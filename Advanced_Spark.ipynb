{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e4fe212-7076-4088-a80b-af8708af908e",
   "metadata": {},
   "source": [
    "#                 ADVANCED CONCEPTS IN PYSPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a59b8150-569b-47fe-9461-2cba95c2b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pyspark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,ArrayType,MapType,DoubleType,BooleanType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.window import Window\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6722fb7-780e-4568-9b60-c4350fff7481",
   "metadata": {},
   "source": [
    "# 1. PySpark - UDF\n",
    "\n",
    "PySpark UDF (a.k.a User Defined Function) is the most useful feature of Spark SQL & DataFrame that is used to extend the PySpark build in capabilities.   \n",
    "\n",
    "UDF’s are used to extend the functions of the framework and re-use these functions on multiple DataFrame’s. For example: you wanted to convert every first letter of a word in a name string to a capital case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84db010f-a101-4f7b-8d59-83cc744e1388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea23fb66-7722-4f3e-a8c9-b8aaf46140ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Python Function\n",
    "\n",
    "def convertCase(str):\n",
    "    resStr=\"\"\n",
    "    arr = str.split(\" \")\n",
    "    for x in arr:\n",
    "       resStr= resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n",
    "    return resStr "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68834c9-a6f7-478b-b598-f100407057be",
   "metadata": {},
   "source": [
    "### Convert python function to PySpark UDF\n",
    "\n",
    "Now convert this function convertCase() to UDF by passing the \n",
    "function to PySpark SQL udf(), this function is available at \n",
    "org.apache.spark.sql.functions.udf package\n",
    "\n",
    "PySpark SQL udf() function returns org.apache.spark.sql.expressions.UserDefinedFunction class object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f622b445-0966-4abe-955f-598ad2d9e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|Seqno|Name         |\n",
      "+-----+-------------+\n",
      "|1    |John Jones   |\n",
      "|2    |Tracey Smith |\n",
      "|3    |Amy Sanders  |\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converting function to UDF \n",
    "convertUDF = udf(lambda z: convertCase(z),StringType())\n",
    "\n",
    "df.select(col(\"Seqno\"), \\\n",
    "    convertUDF(col(\"Name\")).alias(\"Name\") ) \\\n",
    "   .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b8fc94e-dda2-43b0-a157-020c7eaded16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-------------+\n",
      "|Seqno|Name        |Cureated Name|\n",
      "+-----+------------+-------------+\n",
      "|1    |john jones  |JOHN JONES   |\n",
      "|2    |tracey smith|TRACEY SMITH |\n",
      "|3    |amy sanders |AMY SANDERS  |\n",
      "+-----+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using UDF with PySpark DataFrame withColumn()\n",
    "\n",
    "def upperCase(str):\n",
    "    return str.upper()\n",
    "\n",
    "upperCaseUDF = udf(lambda z:upperCase(z), StringType())\n",
    "df.withColumn(\"Cureated Name\", upperCaseUDF(col(\"Name\"))) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64b16fe7-7f4a-4a75-91f1-7eabb51cbe9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|Seqno|Name         |\n",
      "+-----+-------------+\n",
      "|1    |John Jones   |\n",
      "|2    |Tracey Smith |\n",
      "|3    |Amy Sanders  |\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Registering PySpark UDF & use it on SQL\n",
    "\n",
    "#In order to use convertCase() function on PySpark SQL, you need to \n",
    "#      register the function with PySpark by using spark.udf.register().\n",
    "\n",
    "\"\"\" Using UDF on SQL \"\"\"\n",
    "spark.udf.register(\"convertUDF\", convertCase,StringType())\n",
    "df.createOrReplaceTempView(\"NAME_TABLE\")\n",
    "spark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE\") \\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50c27ae4-08d2-4f74-b50b-9d747d83df3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-------------+\n",
      "|Seqno|Name        |Cureated Name|\n",
      "+-----+------------+-------------+\n",
      "|1    |john jones  |JOHN JONES   |\n",
      "|2    |tracey smith|TRACEY SMITH |\n",
      "|3    |amy sanders |AMY SANDERS  |\n",
      "+-----+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating UDF using annotation\n",
    "\n",
    "@udf(returnType=StringType()) \n",
    "def upperCase(str):\n",
    "    return str.upper()\n",
    "\n",
    "df.withColumn(\"Cureated Name\", upperCase(col(\"Name\"))) \\\n",
    ".show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07760e7-9573-46cf-83fe-6a3f26d372a2",
   "metadata": {},
   "source": [
    "### Special Handling\n",
    "\n",
    "**Execution order:**  One thing to aware is in PySpark/Spark does not guarantee the order of evaluation of subexpressions meaning expressions are not guarantee to evaluated left-to-right or in any other fixed order. PySpark reorders the execution for query optimization and planning hence, AND, OR, WHERE and HAVING expression will have side effects.\n",
    "\n",
    "**Handling null check:** UDF’s are error-prone when not designed carefully. for example, when you have a column that contains the value null on some records. Since we are not handling null with UDF function, using this on DataFrame returns below error. Note that in Python None is considered null.\n",
    "\n",
    "**Performance concern using UDF:** UDFs are a black box to PySpark hence it can’t apply optimization and you will lose all the optimization PySpark does on Dataframe/Dataset. When possible you should use Spark SQL built-in functions as these functions provide optimization. Consider creating UDF only when the existing built-in SQL function doesn’t have it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e55c4cd2-6772-4291-aaea-067e86fcd5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|Seqno|Name       |\n",
      "+-----+-----------+\n",
      "|1    |John Jones |\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "No guarantee Name is not null will execute first\n",
    "If convertUDF(Name) like '%John%' execute first then \n",
    "you will get runtime error\n",
    "\"\"\"\n",
    "spark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE \" + \\\n",
    "          \"where Name is not null and convertUDF(Name) like '%John%'\") \\\n",
    ".show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea18e96a-c399-48b3-affe-32e1158eb32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "|4    |NULL        |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Null in the data usecase\n",
    "\n",
    "\"\"\" null check \"\"\"\n",
    "\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\"),\n",
    "    ('4',None)]\n",
    "\n",
    "df2 = spark.createDataFrame(data=data,schema=columns)\n",
    "df2.show(truncate=False)\n",
    "df2.createOrReplaceTempView(\"NAME_TABLE2\")\n",
    "\n",
    "# spark.sql(\"select convertUDF(Name) from NAME_TABLE2\") \\\n",
    "#      .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77d6494e-b41b-44a6-9cd5-0ceb492713a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|_nullsafeUDF(Name)|\n",
      "+------------------+\n",
      "|John Jones        |\n",
      "|Tracey Smith      |\n",
      "|Amy Sanders       |\n",
      "|                  |\n",
      "+------------------+\n",
      "\n",
      "+-----+-----------+\n",
      "|Seqno|Name       |\n",
      "+-----+-----------+\n",
      "|1    |John Jones |\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.udf.register(\"_nullsafeUDF\", lambda str: convertCase(str) if not str is None else \"\" , StringType())\n",
    "\n",
    "spark.sql(\"select _nullsafeUDF(Name) from NAME_TABLE2\") \\\n",
    "     .show(truncate=False)\n",
    "\n",
    "spark.sql(\"select Seqno, _nullsafeUDF(Name) as Name from NAME_TABLE2 \" + \\\n",
    "          \" where Name is not null and _nullsafeUDF(Name) like '%John%'\") \\\n",
    "     .show(truncate=False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25078907-80e7-436d-adf8-4c62b5c4a799",
   "metadata": {},
   "source": [
    "# 2. PySpark - transform()\n",
    "\n",
    "PySpark provides two transform() functions one with DataFrame and another in pyspark.sql.functions.\n",
    "\n",
    "* pyspark.sql.DataFrame.transform() – Available since Spark 3.0\n",
    "* pyspark.sql.functions.transform()\n",
    "\n",
    "### Syntax (DataFrame):\n",
    "\n",
    "DataFrame.transform(func: Callable[[…], DataFrame], *args: Any, **kwargs: Any) → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "* func – Custom function to call.\n",
    "* *args – Arguments to pass to func.\n",
    "* **kwargs – Keyword arguments to pass to func.\n",
    "\n",
    "### Syntax (SQL):\n",
    "\n",
    "pyspark.sql.functions.transform(col, f)\n",
    "\n",
    "* col – ArrayType column\n",
    "* f – Optional. Function to apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d313ad8-c537-4f2c-b679-b53c94c3c475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CourseName: string (nullable = true)\n",
      " |-- fee: long (nullable = true)\n",
      " |-- discount: long (nullable = true)\n",
      "\n",
      "+----------+----+--------+\n",
      "|CourseName|fee |discount|\n",
      "+----------+----+--------+\n",
      "|Java      |4000|5       |\n",
      "|Python    |4600|10      |\n",
      "|Scala     |4100|15      |\n",
      "|Scala     |4500|15      |\n",
      "|PHP       |3000|20      |\n",
      "+----------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "\n",
    "# Prepare Data\n",
    "simpleData = ((\"Java\",4000,5), \\\n",
    "    (\"Python\", 4600,10),  \\\n",
    "    (\"Scala\", 4100,15),   \\\n",
    "    (\"Scala\", 4500,15),   \\\n",
    "    (\"PHP\", 3000,20),  \\\n",
    "  )\n",
    "columns= [\"CourseName\", \"fee\", \"discount\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "220b4597-72b9-438d-94d2-6f1757b402a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom functions\n",
    "\n",
    "# Custom transformation 1\n",
    "def to_upper_str_columns(df):\n",
    "    return df.withColumn(\"CourseName\",upper(df.CourseName))\n",
    "\n",
    "# Custom transformation 2\n",
    "def reduce_price(df,reduceBy):\n",
    "    return df.withColumn(\"new_fee\",df.fee - reduceBy)\n",
    "\n",
    "# Custom transformation 3\n",
    "def apply_discount(df):\n",
    "    return df.withColumn(\"discounted_fee\",  \\\n",
    "             df.new_fee - (df.new_fee * df.discount) / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcf8bf43-4076-405e-ae04-1f89051f7106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+-------+--------------+\n",
      "|CourseName| fee|discount|new_fee|discounted_fee|\n",
      "+----------+----+--------+-------+--------------+\n",
      "|      JAVA|4000|       5|   3000|        2850.0|\n",
      "|    PYTHON|4600|      10|   3600|        3240.0|\n",
      "|     SCALA|4100|      15|   3100|        2635.0|\n",
      "|     SCALA|4500|      15|   3500|        2975.0|\n",
      "|       PHP|3000|      20|   2000|        1600.0|\n",
      "+----------+----+--------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.transform(to_upper_str_columns) \\\n",
    "    .transform(reduce_price,1000) \\\n",
    "    .transform(apply_discount)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ba16eab-d353-48d6-a8cf-782520dc21bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|CourseName|discounted_fee|\n",
      "+----------+--------------+\n",
      "|      JAVA|        2850.0|\n",
      "|    PYTHON|        3240.0|\n",
      "|     SCALA|        2635.0|\n",
      "|     SCALA|        2975.0|\n",
      "|       PHP|        1600.0|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# custom function\n",
    "def select_columns(df):\n",
    "    return df.select(\"CourseName\",\"discounted_fee\")\n",
    "\n",
    "# Chain transformations\n",
    "df2 = df.transform(to_upper_str_columns) \\\n",
    "        .transform(reduce_price,1000) \\\n",
    "        .transform(apply_discount) \\\n",
    "        .transform(select_columns)\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "106ebfa6-0bbe-4041-a9dd-41856ee3194b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Languages1: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- Languages2: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+----------------+------------------+---------------+\n",
      "|            Name|        Languages1|     Languages2|\n",
      "+----------------+------------------+---------------+\n",
      "|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|\n",
      "|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|\n",
      "+----------------+------------------+---------------+\n",
      "\n",
      "+------------------+\n",
      "|        languages1|\n",
      "+------------------+\n",
      "|[JAVA, SCALA, C++]|\n",
      "|[SPARK, JAVA, C++]|\n",
      "|      [CSHARP, VB]|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame with Array\n",
    "data = [\n",
    " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]),\n",
    " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]),\n",
    " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"])\n",
    "]\n",
    "df = spark.createDataFrame(data=data,schema=[\"Name\",\"Languages1\",\"Languages2\"])\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "# using transform() function\n",
    "df.select(transform(\"Languages1\", lambda x: upper(x)).alias(\"languages1\")) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bf5878-6278-4aeb-982c-6b8f3b3ba711",
   "metadata": {},
   "source": [
    "# 3. PySpark - apply()\n",
    "\n",
    "In order to apply a custom function, first you need to create a function and register the function as a UDF. Recent versions of PySpark provide a way to use Pandas API hence, you can also use pyspark.pandas.DataFrame.apply()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "680f6287-7299-487b-ac26-d113bf20d5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Fee  Discount\n",
      "0  20000.0      1000\n",
      "1  25000.0      2500\n",
      "2  30000.0      1500\n",
      "3  22000.0      1200\n",
      "4      NaN      3000\n",
      "---------------------------\n",
      "0    21000.0\n",
      "1    27500.0\n",
      "2    31500.0\n",
      "3    23200.0\n",
      "4        NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "technologies = ({\n",
    "    'Fee' :[20000,25000,30000,22000,np.NaN],\n",
    "    'Discount':[1000,2500,1500,1200,3000]\n",
    "               })\n",
    "# Create a DataFrame\n",
    "psdf = pd.DataFrame(technologies)\n",
    "print(psdf)\n",
    "\n",
    "print('---------------------------')\n",
    "\n",
    "def add(data):\n",
    "   return data[0] + data[1]\n",
    "   \n",
    "addDF = psdf.apply(add,axis=1)\n",
    "print(addDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630ca13a-3f64-415a-944e-24d242c04cc4",
   "metadata": {},
   "source": [
    "# 4. PySpark - map()\n",
    "\n",
    "The map()in PySpark is a transformation function that is used to apply a function/lambda to each element of an RDD (Resilient Distributed Dataset) and return a new RDD consisting of the result. You can use this for simple to complex operations like deriving a new element from exising data, or transforming the data, etc;\n",
    "\n",
    "**key Points:**\n",
    "* DataFrame doesn’t have map() transformation to use with DataFrame; hence, you need to convert DataFrame to RDD first.  \n",
    "* If you have a heavy initialization, use PySpark mapPartitions() transformation instead of map(); as with mapPartitions(), heavy initialization executes only once for each partition instead of every record.  \n",
    "\n",
    "### Syntax:\n",
    "\n",
    "map(f, preservesPartitioning=False)\n",
    "\n",
    "Here’s how the map() transformation works:\n",
    "\n",
    "* **Function Application:** You define a function that you want to apply to each element of the RDD.\n",
    "* **Function Application to RDD:** You call the map() transformation on the RDD and pass the function as an argument to it.\n",
    "* **Transformation Execution:** Spark applies the provided function to each element of the RDD in a distributed manner across the cluster.\n",
    "* **New RDD Creation:** The map() transformation returns a new RDD containing the results of applying the function to each element of the original RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac5cc313-1404-46bc-be23-641f42124f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let’s create an RDD from the list\n",
    "\n",
    "data = [\"Project\",\"Gutenberg’s\",\"Alice’s\",\"Adventures\",\n",
    "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\",\"Adventures\",\n",
    "\"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\"]\n",
    "\n",
    "rdd=spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9a337e6-c739-4d46-8714-c55acdeaf2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Project', 1)\n",
      "('Gutenberg’s', 1)\n",
      "('Alice’s', 1)\n",
      "('Adventures', 1)\n",
      "('in', 1)\n",
      "('Wonderland', 1)\n",
      "('Project', 1)\n",
      "('Gutenberg’s', 1)\n",
      "('Adventures', 1)\n",
      "('in', 1)\n",
      "('Wonderland', 1)\n",
      "('Project', 1)\n",
      "('Gutenberg’s', 1)\n"
     ]
    }
   ],
   "source": [
    "# map() with rdd\n",
    "rdd2=rdd.map(lambda x: (x,1))\n",
    "for element in rdd2.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4d5c6f-df66-4200-adae-9f33808398b6",
   "metadata": {},
   "source": [
    "### PySpark map() Example with DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57976b56-f029-424b-bfda-a4cdb09cdabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+\n",
      "|firstname|lastname|gender|salary|\n",
      "+---------+--------+------+------+\n",
      "|    James|   Smith|     M|    30|\n",
      "|     Anna|    Rose|     F|    41|\n",
      "|   Robert|Williams|     M|    62|\n",
      "+---------+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('James','Smith','M',30),\n",
    "  ('Anna','Rose','F',41),\n",
    "  ('Robert','Williams','M',62), \n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "887fbdf5-77e0-43b8-b3cb-1c2b2dc1b47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+----------+\n",
      "|           name|gender|new_salary|\n",
      "+---------------+------+----------+\n",
      "|    James,Smith|     M|        60|\n",
      "|      Anna,Rose|     F|        82|\n",
      "|Robert,Williams|     M|       124|\n",
      "+---------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Refering columns by index.\n",
    "rdd2=df.rdd.map(lambda x: \n",
    "    (x[0]+\",\"+x[1],x[2],x[3]*2)\n",
    "    )  \n",
    "df2=rdd2.toDF([\"name\",\"gender\",\"new_salary\"]   )\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d4dd3ff-8fe7-4cbf-8d21-a4b496944975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above example refers to the columns by index. \n",
    "#      The below example uses column names.\n",
    "\n",
    "# Referring Column Names\n",
    "rdd2=df.rdd.map(lambda x: \n",
    "    (x[\"firstname\"]+\",\"+x[\"lastname\"],x[\"gender\"],x[\"salary\"]*2)\n",
    "    ) \n",
    "\n",
    "# Referring Column Names\n",
    "rdd2=df.rdd.map(lambda x: \n",
    "    (x.firstname+\",\"+x.lastname,x.gender,x.salary*2)\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5493a2d3-da97-43d7-8b2b-94abb50090ff",
   "metadata": {},
   "source": [
    "### Using custom function on map() transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc9375cd-f8cc-463e-848d-339a3b777a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+----------+\n",
      "|           name|gender|new_salary|\n",
      "+---------------+------+----------+\n",
      "|    James,Smith|     m|        60|\n",
      "|      Anna,Rose|     f|        82|\n",
      "|Robert,Williams|     m|       124|\n",
      "+---------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# By Calling function\n",
    "def func1(x):\n",
    "    firstName=x.firstname\n",
    "    lastName=x.lastname\n",
    "    name=firstName+\",\"+lastName\n",
    "    gender=x.gender.lower()\n",
    "    salary=x.salary*2\n",
    "    return (name,gender,salary)\n",
    "\n",
    "# Apply the func1 function using lambda\n",
    "rdd2 = df.rdd.map(lambda x: func1(x))\n",
    "\n",
    "#or\n",
    "# Apply the func1 function to each element of the RDD using map()\n",
    "rdd2 = df.rdd.map(func1)\n",
    "df2=rdd2.toDF([\"name\",\"gender\",\"new_salary\"])\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e90432f-dc87-4f94-9feb-484117ebc97b",
   "metadata": {},
   "source": [
    "# 5. PySpark - flatMap()\n",
    "\n",
    "PySpark flatMap() is a transformation operation that flattens the RDD/DataFrame (array/map DataFrame columns) after applying the function on every element and returns a new PySpark RDD/DataFrame.\n",
    "\n",
    "### Syntax:\n",
    "\n",
    "flatMap(f, preservesPartitioning=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb7a4f4b-f154-4fcf-9288-47359647e583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Gutenberg’s\n",
      "Alice’s Adventures in Wonderland\n",
      "Project Gutenberg’s\n",
      "Adventures in Wonderland\n",
      "Project Gutenberg’s\n"
     ]
    }
   ],
   "source": [
    "data = [\"Project Gutenberg’s\",\n",
    "        \"Alice’s Adventures in Wonderland\",\n",
    "        \"Project Gutenberg’s\",\n",
    "        \"Adventures in Wonderland\",\n",
    "        \"Project Gutenberg’s\"]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "for element in rdd.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9edbe4d2-3978-4bdd-9eec-b17bfa5b3291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project\n",
      "Gutenberg’s\n",
      "Alice’s\n",
      "Adventures\n",
      "in\n",
      "Wonderland\n",
      "Project\n",
      "Gutenberg’s\n",
      "Adventures\n",
      "in\n",
      "Wonderland\n",
      "Project\n",
      "Gutenberg’s\n"
     ]
    }
   ],
   "source": [
    "rdd2=rdd.flatMap(lambda x: x.split(\" \"))\n",
    "for element in rdd2.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a4e946f-5d09-4867-8a6e-d6a35f8cd152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- col: string (nullable = true)\n",
      "\n",
      "+---------+------+\n",
      "|     name|   col|\n",
      "+---------+------+\n",
      "|    James|  Java|\n",
      "|    James| Scala|\n",
      "|  Michael| Spark|\n",
      "|  Michael|  Java|\n",
      "|  Michael|  NULL|\n",
      "|   Robert|CSharp|\n",
      "|   Robert|      |\n",
      "|Jefferson|     1|\n",
      "|Jefferson|     2|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unfortunately, PySpark DataFame doesn’t have flatMap() transformation \n",
    "#      however, DataFrame has explode() SQL function that is used to \n",
    "#      flatten the column\n",
    "\n",
    "arrayData = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})]\n",
    "df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "df2 = df.select(df.name,explode(df.knownLanguages))\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995fb0b0-b9f4-40f3-ab27-da822344a80c",
   "metadata": {},
   "source": [
    "# 6. PySpark - foreach()\n",
    "\n",
    "PySpark foreach() is an action operation that is available in RDD, DataFram to iterate/loop over each element in the DataFrmae, It is similar to for with advanced concepts. \n",
    "\n",
    "### Syntax:\n",
    "DataFrame.foreach(f)\n",
    "\n",
    "When foreach() applied on PySpark DataFrame, it executes a function specified in for each element of DataFrame. This operation is mainly used if you wanted to manipulate accumulators, save the DataFrame results to RDBMS tables, Kafka topics, and other external sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "718028ac-3517-4c1c-942d-9bcb687d42ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|        Name|\n",
      "+-----+------------+\n",
      "|    1|  john jones|\n",
      "|    2|tracey smith|\n",
      "|    3| amy sanders|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.show()\n",
    "\n",
    "# foreach() Example\n",
    "def f(df):\n",
    "    print(df.Seqno)\n",
    "    \n",
    "df.foreach(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
