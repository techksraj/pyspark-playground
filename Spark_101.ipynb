{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17612a57-cd1b-4d51-bace-d491d6cba609",
   "metadata": {},
   "source": [
    "# 1. PySpark - show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c90480ad-357e-40ac-bcbd-6bf1f362fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,ArrayType,MapType,DoubleType,BooleanType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.window import Window\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92479356-2925-4a8e-a2b6-bdf2ba82253a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|Seqno|               Quote|\n",
      "+-----+--------------------+\n",
      "|    1|Be the change tha...|\n",
      "|    2|Everyone thinks o...|\n",
      "|    3|The purpose of ou...|\n",
      "|    4|            Be cool.|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"Seqno\",\"Quote\"]\n",
    "data = [(\"1\", \"Be the change that you wish to see in the world\"),\n",
    "    (\"2\", \"Everyone thinks of changing the world, but no one thinks of changing himself.\"),\n",
    "    (\"3\", \"The purpose of our lives is to be happy.\"),\n",
    "    (\"4\", \"Be cool.\")]\n",
    "df = spark.createDataFrame(data,columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8ea0f26-1e5a-42fd-9d74-bc22d21c720e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------------------------------+\n",
      "|Seqno|Quote                                                                        |\n",
      "+-----+-----------------------------------------------------------------------------+\n",
      "|1    |Be the change that you wish to see in the world                              |\n",
      "|2    |Everyone thinks of changing the world, but no one thinks of changing himself.|\n",
      "|3    |The purpose of our lives is to be happy.                                     |\n",
      "|4    |Be cool.                                                                     |\n",
      "+-----+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afddea9-4ac1-4710-8345-1f3822ced502",
   "metadata": {},
   "source": [
    "# 2. PySpark - StructType & StructField\n",
    "\n",
    "The StructType and StructField classes in PySpark are used to specify the custom schema to the DataFrame and create complex columns like nested struct, array, and map columns. StructType is a collection of StructField objects that define column name, column data type, boolean to specify if the field can be nullable or not, and metadata\n",
    "\n",
    "* StructType – Defines the structure of the DataFrame\n",
    "* StructField – Defines the metadata of the DataFrame column: column names, data types, and whether they’re nullable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dfe790a-5146-4c90-9a49-fb0ba07a7a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \n",
    "    StructField(\"firstname\",StringType(),True), \n",
    "    StructField(\"middlename\",StringType(),True), \n",
    "    StructField(\"lastname\",StringType(),True), \n",
    "    StructField(\"id\", StringType(), True), \n",
    "    StructField(\"gender\", StringType(), True), \n",
    "    StructField(\"salary\", IntegerType(), True) \n",
    "  ])\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55710ddc-c6e0-4ea8-8e25-2fafa168adfd",
   "metadata": {},
   "source": [
    "### Defining Nested StructType object struct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16bd7fe0-e72e-40e5-bb1a-55141de0b997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+--------------------+-----+------+------+\n",
      "|name                |id   |gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|{James, , Smith}    |36636|M     |3100  |\n",
      "|{Michael, Rose, }   |40288|M     |4300  |\n",
      "|{Robert, , Williams}|42114|M     |1400  |\n",
      "|{Maria, Anne, Jones}|39192|F     |5500  |\n",
      "|{Jen, Mary, Brown}  |     |F     |-1    |\n",
      "+--------------------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "structureData = [\n",
    "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "structureSchema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('id', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', IntegerType(), True)\n",
    "         ])\n",
    "\n",
    "df2 = spark.createDataFrame(data = structureData, schema = structureSchema)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5226a65-2b31-4204-878c-b45ba0d518bb",
   "metadata": {},
   "source": [
    "### Adding & Changing struct of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cad7500e-5083-4c41-9cf8-90a30b9d74a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- OtherInfo: struct (nullable = false)\n",
      " |    |-- identifier: string (nullable = true)\n",
      " |    |-- gender: string (nullable = true)\n",
      " |    |-- salary: integer (nullable = true)\n",
      " |    |-- Salary_Grade: string (nullable = false)\n",
      "\n",
      "+--------------------+------------------------+\n",
      "|name                |OtherInfo               |\n",
      "+--------------------+------------------------+\n",
      "|{James, , Smith}    |{36636, M, 3100, Medium}|\n",
      "|{Michael, Rose, }   |{40288, M, 4300, High}  |\n",
      "|{Robert, , Williams}|{42114, M, 1400, Low}   |\n",
      "|{Maria, Anne, Jones}|{39192, F, 5500, High}  |\n",
      "|{Jen, Mary, Brown}  |{, F, -1, Low}          |\n",
      "+--------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updatedDF = df2.withColumn(\"OtherInfo\", \n",
    "    struct(col(\"id\").alias(\"identifier\"),\n",
    "    col(\"gender\").alias(\"gender\"),\n",
    "    col(\"salary\").alias(\"salary\"),\n",
    "    when(col(\"salary\").cast(IntegerType()) < 2000,\"Low\")\n",
    "      .when(col(\"salary\").cast(IntegerType()) < 4000,\"Medium\")\n",
    "      .otherwise(\"High\").alias(\"Salary_Grade\")\n",
    "  )).drop(\"id\",\"gender\",\"salary\")\n",
    "\n",
    "updatedDF.printSchema()\n",
    "updatedDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf03b4b8-9601-4a29-b4c5-e8a182822096",
   "metadata": {},
   "source": [
    "### Using SQL ArrayType and MapType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7de6091b-1827-430f-a61c-d54d49a466e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayStructureSchema = StructType([\n",
    "    StructField('name', StructType([\n",
    "       StructField('firstname', StringType(), True),\n",
    "       StructField('middlename', StringType(), True),\n",
    "       StructField('lastname', StringType(), True)\n",
    "       ])),\n",
    "       StructField('hobbies', ArrayType(StringType()), True),\n",
    "       StructField('properties', MapType(StringType(),StringType()), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4831153-8eed-472c-a48a-c8904dd406d1",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "root\n",
    " |-- name: struct (nullable = true)  \n",
    " |    |-- firstname: string (nullable = true)  \n",
    " |    |-- middlename: string (nullable = true)  \n",
    " |    |-- lastname: string (nullable = true)  \n",
    " |-- hobbies: array (nullable = true)  \n",
    " |    |-- element: string (containsNull = true)  \n",
    " |-- properties: map (nullable = true)  \n",
    " |    |-- key: string   \n",
    " |    |-- value: string (valueContainsNull = true)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f88f97-4546-41f5-9a7d-c967948991a8",
   "metadata": {},
   "source": [
    "### Creating StructType object struct from JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83e68fae-37e9-4670-98e3-11ff895f0f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"fields\":[{\"metadata\":{},\"name\":\"name\",\"nullable\":true,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"firstname\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"middlename\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"lastname\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}},{\"metadata\":{},\"name\":\"id\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"gender\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"salary\",\"nullable\":true,\"type\":\"integer\"}],\"type\":\"struct\"}\n"
     ]
    }
   ],
   "source": [
    "print(df2.schema.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8439c417-a847-4cba-8a00-3d59c472a287",
   "metadata": {},
   "source": [
    "### Check DataFrame Column Exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "690f449e-974a-44e9-9d87-e5ef53d12fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'firstname' exists in the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "if \"firstname\" in df.columns:\n",
    "    print(\"Column 'firstname' exists in the DataFrame.\")\n",
    "else:\n",
    "    print(\"Column 'firstname' does not exist in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092d6f45-bb62-4d8f-a586-313f8f448c68",
   "metadata": {},
   "source": [
    "# 3. PySpark - Column functions\n",
    "\n",
    "pyspark.sql.Column class provides several functions to work with DataFrame to manipulate the Column values, evaluate the boolean expression to filter rows, retrieve a value or part of a value from a DataFrame column, and to work with list, map & struct columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d906cc5b-a9a7-43d8-9671-ab5420588cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name.fname: string (nullable = true)\n",
      " |-- gender: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"James\",23),(\"Ann\",40)]\n",
    "df=spark.createDataFrame(data).toDF(\"name.fname\",\"gender\")\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7523b09-f56e-40bd-89dc-ad4e9929141d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|    23|\n",
      "|    40|\n",
      "+------+\n",
      "\n",
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|    23|\n",
      "|    40|\n",
      "+------+\n",
      "\n",
      "+----------+\n",
      "|name.fname|\n",
      "+----------+\n",
      "|     James|\n",
      "|       Ann|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using DataFrame object (df)\n",
    "df.select(df.gender).show()\n",
    "df.select(df[\"gender\"]).show()\n",
    "#Accessing column name with dot (with backticks)\n",
    "df.select(df[\"`name.fname`\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "730e0117-d48c-4308-81d5-9f3a169a1ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|    23|\n",
      "|    40|\n",
      "+------+\n",
      "\n",
      "+----------+\n",
      "|name.fname|\n",
      "+----------+\n",
      "|     James|\n",
      "|       Ann|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using SQL col() function\n",
    "df.select(col(\"gender\")).show()\n",
    "#Accessing column name with dot (with backticks)\n",
    "df.select(col(\"`name.fname`\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e5b86bf-4b97-44b0-83c6-9cf20b70d1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- prop: struct (nullable = true)\n",
      " |    |-- hair: string (nullable = true)\n",
      " |    |-- eye: string (nullable = true)\n",
      "\n",
      "+-----+-------------+\n",
      "| name|         prop|\n",
      "+-----+-------------+\n",
      "|James|{black, blue}|\n",
      "|  Ann|{grey, black}|\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create DataFrame with struct using Row class\n",
    "data = [\n",
    "    Row (name=\"James\", prop = Row(hair = \"black\", eye = \"blue\")),\n",
    "    Row (name=\"Ann\", prop = Row(hair = \"grey\", eye = \"black\"))\n",
    "       ]\n",
    "df = spark.createDataFrame(data)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a44c432-3a52-4a73-9604-78dee11ac936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|James|\n",
      "|  Ann|\n",
      "+-----+\n",
      "\n",
      "+-----+\n",
      "| hair|\n",
      "+-----+\n",
      "|black|\n",
      "| grey|\n",
      "+-----+\n",
      "\n",
      "+-----+-----+\n",
      "| hair|  eye|\n",
      "+-----+-----+\n",
      "|black| blue|\n",
      "| grey|black|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('name').show()\n",
    "df.select(df['prop.hair']).show()\n",
    "df.select(col(\"prop.*\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0d13c9e-cf8a-4adc-951d-673263e0d0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| _1| _2| _3|\n",
      "+---+---+---+\n",
      "|100|  2|  1|\n",
      "|200|  3|  4|\n",
      "|300|  4|  4|\n",
      "+---+---+---+\n",
      "\n",
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "| 100|   2|   1|\n",
      "| 200|   3|   4|\n",
      "| 300|   4|   4|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(100,2,1),(200,3,4),(300,4,4)]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n",
    "df = spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fb0c15-9421-4c47-83c0-ad77a7d27603",
   "metadata": {},
   "source": [
    "### PySpark Column functions\n",
    "\n",
    "Column Function\tFunction Description       \n",
    "alias(*alias, **kwargs)       \n",
    "name(*alias, **kwargs)\tProvides alias to the column or expressions       \n",
    "name() returns same as alias().       \n",
    "asc()       \n",
    "asc_nulls_first()       \n",
    "asc_nulls_last()\tReturns ascending order of the column.       \n",
    "asc_nulls_first() Returns null values first then non-null values.       \n",
    "asc_nulls_last() – Returns null values after non-null values.       \n",
    "astype(dataType)       \n",
    "cast(dataType)\tUsed to cast the data type to another type.       \n",
    "astype() returns same as cast().       \n",
    "between(lowerBound, upperBound)\tChecks if the columns values are between lower and upper bound. Returns boolean value.       \n",
    "bitwiseAND(other)       \n",
    "bitwiseOR(other)       \n",
    "bitwiseXOR(other)\tCompute bitwise AND, OR & XOR of this expression with another expression respectively.       \n",
    "contains(other)\tCheck if String contains in another string.       \n",
    "desc()       \n",
    "desc_nulls_first()       \n",
    "desc_nulls_last()\tReturns descending order of the column.       \n",
    "desc_nulls_first() -null values appear before non-null values.       \n",
    "desc_nulls_last() – null values appear after non-null values.       \n",
    "startswith(other)       \n",
    "endswith(other)\tString starts with. Returns boolean expression       \n",
    "String ends with. Returns boolean expression       \n",
    "eqNullSafe(other)\tEquality test that is safe for null values.       \n",
    "getField(name)\tReturns a field by name in a StructField and by key in Map.       \n",
    "getItem(key)\tReturns a values from Map/Key at the provided position.       \n",
    "isNotNull()       \n",
    "isNull()\tisNotNull() – Returns True if the current expression is NOT null.       \n",
    "isNull() – Returns True if the current expression is null.       \n",
    "isin(*cols)\tA boolean expression that is evaluated to true if the value of this expression is contained by the evaluated values of the arguments.       \n",
    "like(other)       \n",
    "rlike(other)\tSimilar to SQL like expression.       \n",
    "Similar to SQL RLIKE expression (LIKE with Regex).       \n",
    "over(window)\tUsed with window column       \n",
    "substr(startPos, length)\tReturn a Column which is a substring of the column.       \n",
    "when(condition, value)       \n",
    "otherwise(value)\tSimilar to SQL CASE WHEN, Executes a list of conditions and returns one of multiple possible result expressions.       \n",
    "dropFields(*fieldNames)\tUsed to drops fields in StructType by name.       \n",
    "withField(fieldName, col)\tAn expression that adds/replaces a field in StructType by name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c188e625-b1c7-4d70-9b45-3b9ea65b22ad",
   "metadata": {},
   "source": [
    "### PySpark column function examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95bf5109-7f7f-4b6c-afe8-1ecd2f6b26c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[(\"James\",\"Bond\",\"100\",None),\n",
    "      (\"Ann\",\"Varsa\",\"200\",'F'),\n",
    "      (\"Tom Cruise\",\"XXX\",\"400\",''),\n",
    "      (\"Tom Brand\",None,\"400\",'M')] \n",
    "columns=[\"fname\",\"lname\",\"id\",\"gender\"]\n",
    "df=spark.createDataFrame(data,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f59e993a-8dde-4afd-8221-da74cffd0494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     fname|\n",
      "+----------+\n",
      "|     James|\n",
      "|       Ann|\n",
      "|Tom Cruise|\n",
      "| Tom Brand|\n",
      "+----------+\n",
      "\n",
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|       Ann|Varsa|200|     F|\n",
      "|     James| Bond|100|  NULL|\n",
      "| Tom Brand| NULL|400|     M|\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "+----------+-----+---+------+\n",
      "\n",
      "+-----+-----+---+------+\n",
      "|fname|lname| id|gender|\n",
      "+-----+-----+---+------+\n",
      "|James| Bond|100|  NULL|\n",
      "|  Ann|Varsa|200|     F|\n",
      "+-----+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#alias\n",
    "df.select(df.fname).alias(\"First_Name\").show()\n",
    "\n",
    "#asc() and dsc()\n",
    "df.sort(df.fname.asc()).show()\n",
    "\n",
    "#between()\n",
    "df.filter(df.id.between(100,300)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22934280-6e42-41ed-8fda-4932f90cad90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+------+\n",
      "|     fname|lname| id|gender|\n",
      "+----------+-----+---+------+\n",
      "|Tom Cruise|  XXX|400|      |\n",
      "+----------+-----+---+------+\n",
      "\n",
      "+-----+-----+---+------+\n",
      "|fname|lname| id|gender|\n",
      "+-----+-----+---+------+\n",
      "|James| Bond|100|  NULL|\n",
      "+-----+-----+---+------+\n",
      "\n",
      "+----------+-----+----------+\n",
      "|     fname|lname|new_gender|\n",
      "+----------+-----+----------+\n",
      "|     James| Bond|      NULL|\n",
      "|       Ann|Varsa|    Female|\n",
      "|Tom Cruise|  XXX|          |\n",
      "| Tom Brand| NULL|      Male|\n",
      "+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#contains\n",
    "df.filter(df.fname.contains(\"Cruise\")).show()\n",
    "\n",
    "#isNull and isNotNull\n",
    "df.filter(df.gender.isNull()).show()\n",
    "\n",
    "#when and otherwise\n",
    "df.select(df.fname, df.lname, when(df.gender==\"M\",\"Male\") \\\n",
    "          .when(df.gender==\"F\",\"Female\") \\\n",
    "          .when(df.gender==None,\"\") \\\n",
    "          .otherwise(df.gender).alias(\"new_gender\") \\\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf64a886-78ad-4041-b728-b1429f5502f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "|fname|lname| id|\n",
      "+-----+-----+---+\n",
      "|James| Bond|100|\n",
      "|  Ann|Varsa|200|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#isin\n",
    "li=[\"100\",\"200\"]\n",
    "df.select(df.fname,df.lname,df.id) \\\n",
    "  .filter(df.id.isin(li)) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dcc010-8dfe-4466-957e-e2ce6a1ef2e8",
   "metadata": {},
   "source": [
    "# 4. PySpark - select columns\n",
    "\n",
    "In PySpark, select() function is used to select single, multiple, column by index, all columns from the list and the nested columns from a DataFrame, PySpark select() is a transformation function hence it returns a new DataFrame with the selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6752b35-96fd-4592-af6d-9439c6a93e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|James    |Smith   |USA    |CA   |\n",
      "|Michael  |Rose    |USA    |NY   |\n",
      "|Robert   |Williams|USA    |CA   |\n",
      "|Maria    |Jones   |USA    |FL   |\n",
      "+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "# Column names\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78960c0d-b664-4eae-bbc4-a0f8effa6301",
   "metadata": {},
   "source": [
    "### Select Single & Multiple Columns From PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "357a5d88-2e47-457b-9229-a9ba6bae8d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select columns by different ways\n",
    "# df.select(\"firstname\",\"lastname\").show()\n",
    "# df.select(df.firstname,df.lastname).show()\n",
    "# df.select(df[\"firstname\"],df[\"lastname\"]).show()\n",
    "# df.select(col(\"firstname\"),col(\"lastname\")).show()\n",
    "\n",
    "# Select columns by regular expression\n",
    "df.select(df.colRegex(\"`^.*name*`\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234fa509-cf6b-4694-aae4-3a5ad7ba2381",
   "metadata": {},
   "source": [
    "### Select All Columns From List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fea78d3e-be3f-49d1-88fd-78f0b08f9830",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select All columns from List\n",
    "df.select(*columns).show()\n",
    "\n",
    "#Select All columns\n",
    "df.select([col for col in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c4c72c-a7b4-478e-9b6d-3dc2488b5ad3",
   "metadata": {},
   "source": [
    "### Select Nested Struct Columns from PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b1b15a2-746e-450e-92aa-11a665e26faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "+----------------------+-----+------+\n",
      "|name                  |state|gender|\n",
      "+----------------------+-----+------+\n",
      "|{James, NULL, Smith}  |OH   |M     |\n",
      "|{Anna, Rose, }        |NY   |F     |\n",
      "|{Julia, , Williams}   |OH   |F     |\n",
      "|{Maria, Anne, Jones}  |NY   |M     |\n",
      "|{Jen, Mary, Brown}    |NY   |M     |\n",
      "|{Mike, Mary, Williams}|OH   |M     |\n",
      "+----------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame with nested columns\n",
    "data = [\n",
    "        ((\"James\",None,\"Smith\"),\"OH\",\"M\"),\n",
    "        ((\"Anna\",\"Rose\",\"\"),\"NY\",\"F\"),\n",
    "        ((\"Julia\",\"\",\"Williams\"),\"OH\",\"F\"),\n",
    "        ((\"Maria\",\"Anne\",\"Jones\"),\"NY\",\"M\"),\n",
    "        ((\"Jen\",\"Mary\",\"Brown\"),\"NY\",\"M\"),\n",
    "        ((\"Mike\",\"Mary\",\"Williams\"),\"OH\",\"M\")\n",
    "        ]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('name', StructType([\n",
    "         StructField('firstname', StringType(), True),\n",
    "         StructField('middlename', StringType(), True),\n",
    "         StructField('lastname', StringType(), True)\n",
    "         ])),\n",
    "     StructField('state', StringType(), True),\n",
    "     StructField('gender', StringType(), True)\n",
    "     ])\n",
    "\n",
    "df2 = spark.createDataFrame(data = data, schema = schema)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False) # shows all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90251452-c4eb-4fea-b367-e5331b899f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|name                  |\n",
      "+----------------------+\n",
      "|{James, NULL, Smith}  |\n",
      "|{Anna, Rose, }        |\n",
      "|{Julia, , Williams}   |\n",
      "|{Maria, Anne, Jones}  |\n",
      "|{Jen, Mary, Brown}    |\n",
      "|{Mike, Mary, Williams}|\n",
      "+----------------------+\n",
      "\n",
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|James    |Smith   |\n",
      "|Anna     |        |\n",
      "|Julia    |Williams|\n",
      "|Maria    |Jones   |\n",
      "|Jen      |Brown   |\n",
      "|Mike     |Williams|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select struct column\n",
    "df2.select(\"name\").show(truncate=False)\n",
    "\n",
    "# Select child columns\n",
    "df2.select(\"name.firstname\",\"name.lastname\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb57bb6b-2ae8-497f-a1e6-849af6928591",
   "metadata": {},
   "source": [
    "# 5.PySpark - where() and filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39e735da-c617-493d-b9d8-2d737d6fc05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- languages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
      "|{Anna, Rose, }        |[Spark, Java, C++]|NY   |F     |\n",
      "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
      "|{Maria, Anne, Jones}  |[CSharp, VB]      |NY   |M     |\n",
      "|{Jen, Mary, Brown}    |[CSharp, VB]      |NY   |M     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create data\n",
    "data = [\n",
    "    ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\"),\n",
    "    ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n",
    "    ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "    ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")\n",
    " ]\n",
    "\n",
    "# Create schema        \n",
    "schema = StructType([\n",
    "     StructField('name', StructType([\n",
    "        StructField('firstname', StringType(), True),\n",
    "        StructField('middlename', StringType(), True),\n",
    "         StructField('lastname', StringType(), True)\n",
    "     ])),\n",
    "     StructField('languages', ArrayType(StringType()), True),\n",
    "     StructField('state', StringType(), True),\n",
    "     StructField('gender', StringType(), True)\n",
    " ])\n",
    "\n",
    "# Create dataframe\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59812b3e-c913-4bc5-a614-814ba9327740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
      "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n",
      "+--------------------+------------------+-----+------+\n",
      "|name                |languages         |state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|{Anna, Rose, }      |[Spark, Java, C++]|NY   |F     |\n",
      "|{Maria, Anne, Jones}|[CSharp, VB]      |NY   |M     |\n",
      "|{Jen, Mary, Brown}  |[CSharp, VB]      |NY   |M     |\n",
      "+--------------------+------------------+-----+------+\n",
      "\n",
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|\n",
      "|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n",
      "|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n",
      "|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using equal condition\n",
    "df.filter(df.state == \"OH\").show(truncate=False)\n",
    "\n",
    "#using not equal condition\n",
    "df.filter(df.state != \"OH\").show(truncate=False)\n",
    "\n",
    "# Using SQL Expression\n",
    "df.filter(\"gender == 'M'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efb206bd-88e2-4697-94f1-51b759b15a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n",
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|\n",
      "| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n",
      "|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter multiple conditions\n",
    "df.filter( (df.state  == \"OH\") & (df.gender  == \"M\") ) \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "#Filter IS IN List values\n",
    "li=[\"OH\",\"CA\",\"DE\"]\n",
    "df.filter(df.state.isin(li)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f0327c-b89a-4b71-89ee-de8377502361",
   "metadata": {},
   "source": [
    "### Filtering with Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45271692-049e-4ace-bd7e-a9680ebb454e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|      name|\n",
      "+---+----------+\n",
      "|  5|Rames rose|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data\n",
    "data2 = [(2,\"Michael Rose\"),(3,\"Robert Williams\"),\n",
    "     (4,\"Rames Rose\"),(5,\"Rames rose\")\n",
    "  ]\n",
    "\n",
    "df2 = spark.createDataFrame(data=data2, schema=[\"id\",\"name\"])\n",
    "\n",
    "# like - SQL LIKE pattern\n",
    "df2.filter(df2.name.like(\"%rose%\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b307e0-65ff-41c7-85d9-42c1b859ecf9",
   "metadata": {},
   "source": [
    "### Filtering Array column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c1a95b0-1414-49aa-ae44-065e6429df8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+-----+------+\n",
      "|            name|         languages|state|gender|\n",
      "+----------------+------------------+-----+------+\n",
      "|{James, , Smith}|[Java, Scala, C++]|   OH|     M|\n",
      "|  {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n",
      "+----------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(array_contains(df.languages,\"Java\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5b9d8d7-0816-4e3e-885a-68d63f225009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------+-----+------+\n",
      "|name                  |languages   |state|gender|\n",
      "+----------------------+------------+-----+------+\n",
      "|{Julia, , Williams}   |[CSharp, VB]|OH   |F     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]|OH   |M     |\n",
      "+----------------------+------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Struct condition (Nested)\n",
    "df.filter(df.name.lastname == \"Williams\") \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f34bad5-0546-4c7d-a1c6-be36705a1693",
   "metadata": {},
   "source": [
    "# 6. PySpark orderBy() and sort()\n",
    "\n",
    "Syntax: DataFrame.sort(*cols, **kwargs) and DataFrame.orderBy(*cols, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7851046a-afc9-4654-8892-a3e23fef85be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000), \\\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "# Create SparkSession\n",
    "\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d365c843-44a2-40a9-a5bb-7e8923e9017a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"department\",\"state\").show()\n",
    "\n",
    "df.sort(df.department.asc(),df.state.desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35614d01-979c-4a80-8cb5-4101b617140c",
   "metadata": {},
   "source": [
    "### Using raw sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c75abddb-3bd7-4ea8-83e2-3bc3c830bc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"EMP\")\n",
    "spark.sql(\"select employee_name,department,state,salary,age,bonus from EMP ORDER BY department asc\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34b98020-916c-436c-a8f9-31b29c067cd4",
   "metadata": {},
   "source": [
    "# 7. GroupBy \n",
    "\n",
    "### Syntax\n",
    "DataFrame.groupBy(*cols)  \n",
    "or  \n",
    "DataFrame.groupby(*cols) \n",
    "\n",
    "### Function\tDefinition\n",
    "count() -\tUse groupBy() count() to return the number of rows for each group.   \n",
    "mean() -\tReturns the mean of values for each group.   \n",
    "max() -\tReturns the maximum of values for each group.   \n",
    "min() -\tReturns the minimum of values for each group   \n",
    "sum() -\tReturns the total for values for each group.   \n",
    "avg() -\tReturns the average for values for each group.   \n",
    "agg() -\tUsing groupBy() agg() function, we can calculate more than one aggregate at a time.   \n",
    "pivot() -\tThis function is used to Pivot the DataFrame, which I will not cover in this article as I already have a dedicated article for Pivot & Unpivot DataFrame.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6394429f-b4a1-410a-9687-08f1d8183e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "# Create DataFrame\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74129ea2-dae7-40b8-b59c-46a4ed07b226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|sum(salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|     257000|\n",
      "|   Finance|     351000|\n",
      "| Marketing|     171000|\n",
      "+----------+-----------+\n",
      "\n",
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|     Sales|    3|\n",
      "|   Finance|    4|\n",
      "| Marketing|    2|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----------+\n",
      "|department|min(salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|      81000|\n",
      "|   Finance|      79000|\n",
      "| Marketing|      80000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using groupBy().sum()\n",
    "df.groupBy(\"department\").sum(\"salary\").show()\n",
    "\n",
    "# Using groupBy().count()\n",
    "df.groupBy(col(\"department\")).count().show()\n",
    "\n",
    "# Using groupBy().min()\n",
    "df.groupBy(df['department']).min(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d11e04-b2c8-4947-9b56-b75e308e9cb9",
   "metadata": {},
   "source": [
    "### Using Multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fcd4cc35-0d6a-4696-94a7-0a5477d5b83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+----------+\n",
      "|department|state|sum(salary)|sum(bonus)|\n",
      "+----------+-----+-----------+----------+\n",
      "|     Sales|   NY|     176000|     30000|\n",
      "|     Sales|   CA|      81000|     23000|\n",
      "|   Finance|   CA|     189000|     47000|\n",
      "|   Finance|   NY|     162000|     34000|\n",
      "| Marketing|   NY|      91000|     21000|\n",
      "| Marketing|   CA|      80000|     18000|\n",
      "+----------+-----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GroupBy on multiple columns\n",
    "df.groupBy(\"department\",\"state\").sum(\"salary\",\"bonus\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c56fa2-32a6-4ab4-8114-b0836268c600",
   "metadata": {},
   "source": [
    "### Running more aggregates at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b2daa1df-5b7e-450e-8434-13204649c2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+---------+------------------+\n",
      "|department|sum_salary|       avg_salary|sum_bonus|         avg_bonus|\n",
      "+----------+----------+-----------------+---------+------------------+\n",
      "|     Sales|    257000|85666.66666666667|    53000|17666.666666666668|\n",
      "|   Finance|    351000|          87750.0|    81000|           20250.0|\n",
      "| Marketing|    171000|          85500.0|    39000|           19500.0|\n",
      "+----------+----------+-----------------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running more aggregations\n",
    "df.groupBy(col(\"department\")).agg(\n",
    "    sum(\"salary\").alias(\"sum_salary\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    sum(\"bonus\").alias(\"sum_bonus\"),\n",
    "    avg(\"bonus\").alias(\"avg_bonus\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea035f8-6d8b-44ab-8d28-89f73772d5ce",
   "metadata": {},
   "source": [
    "### Using filter on aggregate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3b5639c-6819-4784-ab89-ff82fc764edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+---------+------------------+\n",
      "|department|sum_salary|       avg_salary|sum_bonus|         avg_bonus|\n",
      "+----------+----------+-----------------+---------+------------------+\n",
      "|     Sales|    257000|85666.66666666667|    53000|17666.666666666668|\n",
      "|   Finance|    351000|          87750.0|    81000|           20250.0|\n",
      "+----------+----------+-----------------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(col(\"department\")).agg(\n",
    "    sum(\"salary\").alias(\"sum_salary\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    sum(\"bonus\").alias(\"sum_bonus\"),\n",
    "    avg(\"bonus\").alias(\"avg_bonus\")\n",
    ").filter(col(\"sum_bonus\") > 50000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455115cd-3c3e-448a-8645-c8bd220658f6",
   "metadata": {},
   "source": [
    "### PySpark SQL GROUP BY Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eae9d0d4-f5ff-419f-adcf-c4e215baaa2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+---------+------------------+\n",
      "|department|sum_salary|       avg_salary|sum_bonus|         avg_bonus|\n",
      "+----------+----------+-----------------+---------+------------------+\n",
      "|     Sales|    257000|85666.66666666667|    53000|17666.666666666668|\n",
      "|   Finance|    351000|          87750.0|    81000|           20250.0|\n",
      "+----------+----------+-----------------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "sql_string = \"\"\"\n",
    "select department,sum(salary) as sum_salary, avg(salary) as avg_salary,\n",
    "        sum(bonus) as sum_bonus, avg(bonus) as avg_bonus\n",
    "from employees\n",
    "group by department\n",
    "having sum(bonus)>50000\n",
    "\"\"\"\n",
    "df2 = spark.sql(sql_string)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5295db1a-82a5-4bf1-ad50-f8f84edbbc8a",
   "metadata": {},
   "source": [
    "# 8. PySpark - Join()\n",
    "\n",
    "PySpark Join is used to combine two DataFrames and by chaining these you can join multiple DataFrames; it supports all basic join type operations available in traditional SQL like INNER, LEFT OUTER, RIGHT OUTER, LEFT ANTI, LEFT SEMI, CROSS, SELF JOIN. PySpark Joins are wider transformations that involve data shuffling across the network.\n",
    "\n",
    "\n",
    "### Syntax\n",
    "join(self, other, on=None, how=None)\n",
    "\n",
    "join() operation takes parameters as below and returns DataFrame.\n",
    "\n",
    "* param other: Right side of the join\n",
    "* param on: a string for the join column name\n",
    "* param how: default inner. Must be one of inner, cross, outer,full, full_outer, left, left_outer, right, right_outer,left_semi, and left_anti.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ca0cd8e-212d-4355-a7c8-593d61719db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- superior_emp_id: long (nullable = true)\n",
      " |-- year_joined: string (nullable = true)\n",
      " |-- emp_dept_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n",
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show(truncate=False)\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0f9114-5c7e-468a-9b16-ee9806983126",
   "metadata": {},
   "source": [
    "### How join works?\n",
    "\n",
    "* **Common Key**: In order to join two or more datasets we need a common key or a column on which you want to join. This key is used to join the matching rows from the datasets.\n",
    "\n",
    "* **Partitioning**: PySpark Datasets are distributed and partitioned across multiple nodes in a cluster. Ideally, data with the same join key should be located in the same partition. If the Datasets are not already partitioned on the join key, PySpark may perform a shuffle operation to redistribute the data, ensuring that rows with the same join key are on the same node. Shuffling can be an expensive operation, especially for large Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c1b4b75-ad10-4bc0-a4ea-48b3570d24fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id, \"inner\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e95a30b0-6dc6-42d8-a95d-5c6f1a7aac8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id, \"leftouter\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d50b75ef-4e7b-4cfc-91a3-cb2f8800680f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id, \"rightouter\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1f6f5a58-bd80-4430-a256-ce84f60d5037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id, \"fullouter\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0066f60e-1223-4de0-af6a-eafeb192bae7",
   "metadata": {},
   "source": [
    "### Left Semi Join\n",
    "\n",
    "A Left Semi Join in PySpark returns only the rows from the left DataFrame (the first DataFrame mentioned in the join operation) where there is a match with the right DataFrame (the second DataFrame). It does not include any columns from the right DataFrame in the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "907e304f-4ed9-46c6-a15c-9efdab571dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id, \"leftsemi\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40761335-cf51-4c70-981b-542a0801c6a6",
   "metadata": {},
   "source": [
    "### Left Anti Join\n",
    "\n",
    "A Left Anti Join in PySpark returns only the rows from the left DataFrame (the first DataFrame mentioned in the join operation) where there is no match with the right DataFrame (the second DataFrame). It excludes any rows from the left DataFrame that have a corresponding key in the right DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3141d918-3853-47e1-80a9-de08fe44a81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "|6     |Brown|2              |2010       |50         |      |-1    |\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id, \"leftanti\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2aaaff8f-2acf-495f-8df2-156b06d0e072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------------+\n",
      "|emp_id|    name|superior_emp_id|superior_emp_name|\n",
      "+------+--------+---------------+-----------------+\n",
      "|     2|    Rose|              1|            Smith|\n",
      "|     3|Williams|              1|            Smith|\n",
      "|     4|   Jones|              2|             Rose|\n",
      "|     5|   Brown|              2|             Rose|\n",
      "|     6|   Brown|              2|             Rose|\n",
      "+------+--------+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#self join \n",
    "empDF.alias(\"emp1\").join(empDF.alias(\"emp2\"),\n",
    "    col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"), \"inner\").select(col(\"emp1.emp_id\"),col(\"emp1.name\"), \n",
    "      col(\"emp2.emp_id\").alias(\"superior_emp_id\"), \n",
    "      col(\"emp2.name\").alias(\"superior_emp_name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1683a2fb-2497-47b8-abf5-ad13f51c1ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SQL Expression\n",
    "\n",
    "empDF.createOrReplaceTempView(\"EMP\")\n",
    "deptDF.createOrReplaceTempView(\"DEPT\")\n",
    "\n",
    "joinDF = spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\") \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "joinDF2 = spark.sql(\"select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id\") \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "81b59969-d4fe-4170-8a01-c1124ebcbb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join on multiple dataFrames\n",
    "\n",
    "# df1.join(df2,df1.id1 == df2.id2,\"inner\") \\\n",
    "#    .join(df3,df1.id1 == df3.id3,\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f43e7d-00c4-4551-a5cd-4f13cc71b27b",
   "metadata": {},
   "source": [
    "# 9. union() and unionall()\n",
    "\n",
    "### Syntax\n",
    "dataFrame1.union(dataFrame2)\n",
    "dataFrame1.unionAll(dataFrame2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "860ba533-a1a6-474e-8fb9-597293c30eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000) \\\n",
    "  ]\n",
    "\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5783d0e1-b635-4071-973a-bab2033d2b01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame2\n",
    "simpleData2 = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "columns2= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "\n",
    "df2 = spark.createDataFrame(data = simpleData2, schema = columns2)\n",
    "\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f1e4eac-540a-4206-a426-284db7837636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# union() to merge two DataFrames\n",
    "unionDF = df.union(df2)\n",
    "unionDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a66de7a3-c845-448a-9c85-b2a7ec4c0ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unionAll() to merge two DataFrames\n",
    "unionAllDF = df.unionAll(df2)\n",
    "unionAllDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "13eece45-2c60-4042-84b8-d3c5f51db05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates after union() using distinct()\n",
    "disDF = df.union(df2).distinct()\n",
    "disDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cba9a6-4ab2-428b-8260-2b8939f596a9",
   "metadata": {},
   "source": [
    "# 10. PySpark - DataFrame.transform() and sql.functions.transform()\n",
    "\n",
    "* The pyspark.sql.DataFrame.transform() is used to chain the custom transformations and this function returns the new DataFrame after applying the specified transformations.\n",
    "\n",
    "### Syntax:  \n",
    "DataFrame.transform(func: Callable[[…], DataFrame], *args: Any, **kwargs: Any) → pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "* The PySpark sql.functions.transform() is used to apply the transformation on a column of type Array. This function applies the specified transformation on every element of the array and returns an object of ArrayType.\n",
    "\n",
    "### Syntax:  \n",
    "pyspark.sql.functions.transform(col, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0cc9f8b8-06e6-42ae-b471-c80388f3b5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CourseName: string (nullable = true)\n",
      " |-- fee: long (nullable = true)\n",
      " |-- discount: long (nullable = true)\n",
      "\n",
      "+----------+----+--------+\n",
      "|CourseName|fee |discount|\n",
      "+----------+----+--------+\n",
      "|Java      |4000|5       |\n",
      "|Python    |4600|10      |\n",
      "|Scala     |4100|15      |\n",
      "|Scala     |4500|15      |\n",
      "|PHP       |3000|20      |\n",
      "+----------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data\n",
    "simpleData = ((\"Java\",4000,5), \\\n",
    "    (\"Python\", 4600,10),  \\\n",
    "    (\"Scala\", 4100,15),   \\\n",
    "    (\"Scala\", 4500,15),   \\\n",
    "    (\"PHP\", 3000,20),  \\\n",
    "  )\n",
    "columns= [\"CourseName\", \"fee\", \"discount\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56b7ac5c-6663-4954-a24c-3c5d6f20f40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+-------+--------------+\n",
      "|CourseName| fee|discount|new_fee|discounted_fee|\n",
      "+----------+----+--------+-------+--------------+\n",
      "|      JAVA|4000|       5|   3000|        2850.0|\n",
      "|    PYTHON|4600|      10|   3600|        3240.0|\n",
      "|     SCALA|4100|      15|   3100|        2635.0|\n",
      "|     SCALA|4500|      15|   3500|        2975.0|\n",
      "|       PHP|3000|      20|   2000|        1600.0|\n",
      "+----------+----+--------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PySpark DataFrame.transform()\n",
    "\n",
    "# Custom transformation 1\n",
    "def to_upper_str_columns(df):\n",
    "    return df.withColumn(\"CourseName\",upper(df.CourseName))\n",
    "\n",
    "# Custom transformation 2\n",
    "def reduce_price(df,reduceBy):\n",
    "    return df.withColumn(\"new_fee\",df.fee - reduceBy)\n",
    "\n",
    "# Custom transformation 3\n",
    "def apply_discount(df):\n",
    "    return df.withColumn(\"discounted_fee\",  \\\n",
    "             df.new_fee - (df.new_fee * df.discount) / 100)\n",
    "\n",
    "# PySpark transform() Usage\n",
    "df2 = df.transform(to_upper_str_columns) \\\n",
    "        .transform(reduce_price,1000) \\\n",
    "        .transform(apply_discount)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dee77b1e-8339-4b89-adac-91a110dbb6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Languages1: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- Languages2: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+----------------+------------------+---------------+\n",
      "|            Name|        Languages1|     Languages2|\n",
      "+----------------+------------------+---------------+\n",
      "|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|\n",
      "|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|\n",
      "+----------------+------------------+---------------+\n",
      "\n",
      "+------------------+\n",
      "|        languages1|\n",
      "+------------------+\n",
      "|[JAVA, SCALA, C++]|\n",
      "|[SPARK, JAVA, C++]|\n",
      "|      [CSHARP, VB]|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PySpark sql.functions.transform()\n",
    "\n",
    "# Create DataFrame with Array\n",
    "data = [\n",
    " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]),\n",
    " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]),\n",
    " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"])\n",
    "]\n",
    "df = spark.createDataFrame(data=data,schema=[\"Name\",\"Languages1\",\"Languages2\"])\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "# using transform() function\n",
    "df.select(transform(\"Languages1\", lambda x: upper(x)).alias(\"languages1\")) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9567f5c4-cd8e-4a80-acdf-26d96ec9850e",
   "metadata": {},
   "source": [
    "# 11. PySpark - fillna() and fill()\n",
    "\n",
    "In PySpark,fillna() from DataFrame class or fill() from DataFrameNaFunctions is used to replace NULL/None values on all or selected multiple columns with either zero(0), empty string, space, or any constant literal values.\n",
    "\n",
    "As part of the cleanup, sometimes you may need to Drop Rows with NULL/None Values in PySpark DataFrame and Filter Rows by checking IS NULL/NOT NULL conditions.\n",
    "\n",
    "### Syntax:  \n",
    "fillna(value, subset=None)  \n",
    "fill(value, subset=None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8dbf6711-d90f-4d23-9f23-976612d1197a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- zipcode: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      "\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|id |zipcode|type    |city               |state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|1  |704    |STANDARD|NULL               |PR   |30100     |\n",
      "|2  |704    |NULL    |PASEO COSTA DEL SUR|PR   |NULL      |\n",
      "|3  |709    |NULL    |BDA SAN LUIS       |PR   |3700      |\n",
      "|4  |76166  |UNIQUE  |CINGULAR WIRELESS  |TX   |84000     |\n",
      "|5  |76177  |STANDARD|NULL               |TX   |NULL      |\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filePath=\"small_zipcode.csv\"\n",
    "df = spark.read.options(header='true', inferSchema='true') \\\n",
    "          .csv(filePath)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "93a498a9-3072-4cd8-bd45-bf641aefeee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|               NULL|   PR|     30100|\n",
      "|  2|    704|    NULL|PASEO COSTA DEL SUR|   PR|         0|\n",
      "|  3|    709|    NULL|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|               NULL|   TX|         0|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|               NULL|   PR|     30100|\n",
      "|  2|    704|    NULL|PASEO COSTA DEL SUR|   PR|         0|\n",
      "|  3|    709|    NULL|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|               NULL|   TX|         0|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|                   |   PR|     30100|\n",
      "|  2|    704|        |PASEO COSTA DEL SUR|   PR|      NULL|\n",
      "|  3|    709|        |       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|                   |   TX|      NULL|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Replace 0 for null for all integer columns\n",
    "df.na.fill(value=0).show()\n",
    "\n",
    "#Replace 0 for null on only population column \n",
    "df.na.fill(value=0,subset=[\"population\"]).show()\n",
    "\n",
    "#Replace null with empty string \"\" for all string columns\n",
    "df.na.fill(\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d7b738bb-86eb-4fe6-874d-16745c019dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|            unknown|   PR|     30100|\n",
      "|  2|    704|        |PASEO COSTA DEL SUR|   PR|      NULL|\n",
      "|  3|    709|        |       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|            unknown|   TX|      NULL|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(\"unknown\",[\"city\"]).na.fill(\"\",[\"type\"]).show()\n",
    "\n",
    "#the above statement can also be written like this\n",
    "#df.na.fill({\"city\": \"unknown\", \"type\": \"\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bf07df-81dc-4570-9934-845ad5144754",
   "metadata": {},
   "source": [
    "# 12. PySpark - pivot()\n",
    "\n",
    "PySpark pivot() function is used to rotate/transpose the data from one column into multiple Dataframe columns and back using unpivot(). Pivot() It is an aggregation where one of the grouping columns values is transposed into individual columns with distinct data\n",
    "\n",
    "### Syntax:\n",
    "pivot_df = original_df.groupBy(\"grouping_column\").pivot(\"pivot_column\").agg({\"agg_column\": \"agg_function\"})\n",
    "\n",
    "* grouping_column: The column used for grouping.\n",
    "\n",
    "* pivot_column: The column whose distinct values become new columns.\n",
    "\n",
    "* agg_column: The column for which aggregation is applied (e.g., using a function like sum, avg, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "01dc86f8-ed97-4532-92ec-0abad32d1b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Amount: long (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+-------+------+-------+\n",
      "|Product|Amount|Country|\n",
      "+-------+------+-------+\n",
      "|Banana |1000  |USA    |\n",
      "|Carrots|1500  |USA    |\n",
      "|Beans  |1600  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Banana |400   |China  |\n",
      "|Carrots|1200  |China  |\n",
      "|Beans  |1500  |China  |\n",
      "|Orange |4000  |China  |\n",
      "|Banana |2000  |Canada |\n",
      "|Carrots|2000  |Canada |\n",
      "|Beans  |2000  |Mexico |\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n",
    "      (\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n",
    "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n",
    "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n",
    "\n",
    "columns= [\"Product\",\"Amount\",\"Country\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7fec84d2-4e96-4749-89e6-a052238ecd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Banana: long (nullable = true)\n",
      " |-- Beans: long (nullable = true)\n",
      " |-- Carrots: long (nullable = true)\n",
      " |-- Orange: long (nullable = true)\n",
      "\n",
      "+-------+------+-----+-------+------+\n",
      "|Country|Banana|Beans|Carrots|Orange|\n",
      "+-------+------+-----+-------+------+\n",
      "|China  |400   |1500 |1200   |4000  |\n",
      "|USA    |1000  |1600 |1500   |2000  |\n",
      "|Mexico |NULL  |2000 |NULL   |NULL  |\n",
      "|Canada |2000  |NULL |2000   |NULL  |\n",
      "+-------+------+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivotDF = df.groupBy(\"Country\").pivot(\"Product\").sum(\"Amount\")\n",
    "pivotDF.printSchema()\n",
    "pivotDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a78fca63-0a53-4e36-a74a-fce7f05e295f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Canada: long (nullable = true)\n",
      " |-- China: long (nullable = true)\n",
      " |-- Mexico: long (nullable = true)\n",
      " |-- USA: long (nullable = true)\n",
      "\n",
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico|USA |\n",
      "+-------+------+-----+------+----+\n",
      "|Orange |NULL  |4000 |NULL  |2000|\n",
      "|Beans  |NULL  |1500 |2000  |1600|\n",
      "|Banana |2000  |400  |NULL  |1000|\n",
      "|Carrots|2000  |1200 |NULL  |1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivotDF = df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\n",
    "pivotDF.printSchema()\n",
    "pivotDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1edb9f70-0c25-4d1e-a5ef-78a81a417439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Product|Country|Total|\n",
      "+-------+-------+-----+\n",
      "|Orange |China  |4000 |\n",
      "|Orange |USA    |2000 |\n",
      "|Beans  |China  |1500 |\n",
      "|Beans  |Mexico |2000 |\n",
      "|Beans  |USA    |1600 |\n",
      "|Banana |Canada |2000 |\n",
      "|Banana |China  |400  |\n",
      "|Banana |USA    |1000 |\n",
      "|Carrots|Canada |2000 |\n",
      "|Carrots|China  |1200 |\n",
      "|Carrots|USA    |1500 |\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Applying unpivot()\n",
    "unpivotExpr = \"stack(4, 'Canada', Canada, 'China', China, 'Mexico', Mexico, 'USA', USA) as (Country,Total)\"\n",
    "unPivotDF = pivotDF.select(\"Product\", expr(unpivotExpr)).where(\"Total is not null\")\n",
    "unPivotDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f274260-9c11-4d09-8ee5-82b7b17c94b0",
   "metadata": {},
   "source": [
    "# 13. PySpark - partitionBy()\n",
    "\n",
    "PySpark supports partition in two ways; partition in memory (DataFrame) and partition on the disk (File system).\n",
    "\n",
    "**Partition in memory:** You can partition or repartition the DataFrame by calling repartition() or coalesce() transformations.\n",
    "\n",
    "**Partition on disk:** While writing the PySpark DataFrame back to disk, you can choose how to partition the data based on columns using partitionBy() of pyspark.sql.DataFrameWriter. This is similar to Hives partitions scheme.\n",
    "\n",
    "Below are some of the advantages of using PySpark partitions on memory or on disk.\n",
    "\n",
    "* Fast access to the data\n",
    "* Provides the ability to perform an operation on a smaller dataset  \n",
    "  \n",
    "Partition at rest (disk) is a feature of many databases and data processing frameworks and it is key to make jobs work at scale.\n",
    "\n",
    "### Syntax:\n",
    "\n",
    "partitionBy(self, *cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7c528af1-5811-4395-91c0-101552a9d7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      "\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "|RecordNumber|Country|City               |Zipcode|State|\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "|1           |US     |PARC PARQUE        |704    |PR   |\n",
      "|2           |US     |PASEO COSTA DEL SUR|704    |PR   |\n",
      "|10          |US     |BDA SAN LUIS       |709    |PR   |\n",
      "|49347       |US     |HOLT               |32564  |FL   |\n",
      "|49348       |US     |HOMOSASSA          |34487  |FL   |\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame by reading CSV file\n",
    "df = spark.read.option(\"header\", True).csv(\"simple-zipcodes.csv\")\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6be69a0f-322c-4f3b-bea0-447f04d67c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.option(\"header\",True) \\\n",
    "    .partitionBy(\"state\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"zipcodes-state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "21d52987-5572-497a-95bd-8b100adafabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#partitionBy multiple columns\n",
    "\n",
    "df.write.option(\"header\",True) \\\n",
    "    .partitionBy(\"state\",\"city\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"zipcodes-state-city\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8a186e-a027-42cd-a25b-4878c7baf81b",
   "metadata": {},
   "source": [
    "### Data skew\n",
    "\n",
    "Use option maxRecordsPerFile if you want to control the number of records for each partition. This is particularly helpful when your data is **skewed (Having some partitions with very low records and other partitions with a high number of records)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bc1bdaaf-afa7-4598-a7e5-fedae14538c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#partitionBy() control number of partitions\n",
    "df.write.option(\"header\",True) \\\n",
    "        .option(\"maxRecordsPerFile\", 2) \\\n",
    "        .partitionBy(\"state\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .csv(\"zipcodes-state-skewed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dc8ce0-414a-494b-9ca6-3733ce51607a",
   "metadata": {},
   "source": [
    "### SQL - Read partition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "01376f67-a99d-4b9b-bda9-362909a69930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------+-----+-----------+\n",
      "|RecordNumber|Country|Zipcode|state|       city|\n",
      "+------------+-------+-------+-----+-----------+\n",
      "|       54355|     US|  35146|   AL|SPRINGVILLE|\n",
      "+------------+-------+-------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paraqDF = spark.read.option(\"header\",True) \\\n",
    "    .csv(\"zipcodes-state-city\")\n",
    "paraqDF.createOrReplaceTempView(\"ZIPCODE\")\n",
    "spark.sql(\"select * from ZIPCODE where state = 'AL' and city = 'SPRINGVILLE'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0085d4-ab13-41d9-bff8-94e5775ebed6",
   "metadata": {},
   "source": [
    "# 14. PySpark SQL - Aggregate Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82513bde-fa89-4c29-af08-ae39d60dead0",
   "metadata": {},
   "source": [
    "**PySpark Aggregate Functions**\n",
    "PySpark SQL Aggregate functions are grouped as “agg_funcs” in Pyspark. Below is a list of functions defined under this group. Click on each link to learn with example.\n",
    "\n",
    "* approx_count_distinct\n",
    "* avg\n",
    "* collect_list\n",
    "* collect_set\n",
    "* countDistinct\n",
    "* count\n",
    "* grouping\n",
    "* first\n",
    "* last\n",
    "* kurtosis\n",
    "* max\n",
    "* min\n",
    "* mean\n",
    "* skewness\n",
    "* stddev\n",
    "* stddev_pop\n",
    "* stddev_samp\n",
    "* sum\n",
    "* sumDistinct\n",
    "* variance, var_samp, var_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "17578ea2-aa97-42c9-97b6-489eae17d216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|        James|     Sales|  3000|\n",
      "|      Michael|     Sales|  4600|\n",
      "|       Robert|     Sales|  4100|\n",
      "|        Maria|   Finance|  3000|\n",
      "|        James|     Sales|  3000|\n",
      "|        Scott|   Finance|  3300|\n",
      "|          Jen|   Finance|  3900|\n",
      "|         Jeff| Marketing|  3000|\n",
      "|        Kumar| Marketing|  2000|\n",
      "|         Saif|     Sales|  4100|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  ]\n",
    "schema = [\"employee_name\",\"department\",\"salary\"]\n",
    "df = spark.createDataFrame(data=simpleData,schema=schema)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4736f996-66a5-4e97-a98c-02b89af749c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg: 3400.0\n",
      "+------------------------------------------------------------+\n",
      "|collect_list(salary)                                        |\n",
      "+------------------------------------------------------------+\n",
      "|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n",
      "+------------------------------------------------------------+\n",
      "\n",
      "+------------------------------------+\n",
      "|collect_set(salary)                 |\n",
      "+------------------------------------+\n",
      "|[4600, 3000, 3900, 4100, 3300, 2000]|\n",
      "+------------------------------------+\n",
      "\n",
      "+----------+----+--------+-------+--------------+\n",
      "|CourseName|fee |discount|new_fee|discounted_fee|\n",
      "+----------+----+--------+-------+--------------+\n",
      "|JAVA      |4000|5       |3000   |2850.0        |\n",
      "|PYTHON    |4600|10      |3600   |3240.0        |\n",
      "|SCALA     |4100|15      |3100   |2635.0        |\n",
      "|SCALA     |4500|15      |3500   |2975.0        |\n",
      "|PHP       |3000|20      |2000   |1600.0        |\n",
      "+----------+----+--------+-------+--------------+\n",
      "\n",
      "Distinct Count of Department & Salary: JAVA\n",
      "count: Row(count(salary)=10)\n",
      "+-------------+\n",
      "|first(salary)|\n",
      "+-------------+\n",
      "|3000         |\n",
      "+-------------+\n",
      "\n",
      "+------------+\n",
      "|last(salary)|\n",
      "+------------+\n",
      "|4100        |\n",
      "+------------+\n",
      "\n",
      "+-------------------+\n",
      "|kurtosis(salary)   |\n",
      "+-------------------+\n",
      "|-0.6467803030303032|\n",
      "+-------------------+\n",
      "\n",
      "+-----------+\n",
      "|max(salary)|\n",
      "+-----------+\n",
      "|4600       |\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|min(salary)|\n",
      "+-----------+\n",
      "|2000       |\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|avg(salary)|\n",
      "+-----------+\n",
      "|3400.0     |\n",
      "+-----------+\n",
      "\n",
      "+--------------------+\n",
      "|skewness(salary)    |\n",
      "+--------------------+\n",
      "|-0.12041791181069571|\n",
      "+--------------------+\n",
      "\n",
      "+-----------------+-------------------+------------------+\n",
      "|stddev(salary)   |stddev_samp(salary)|stddev_pop(salary)|\n",
      "+-----------------+-------------------+------------------+\n",
      "|765.9416862050705|765.9416862050705  |726.636084983398  |\n",
      "+-----------------+-------------------+------------------+\n",
      "\n",
      "+-----------+\n",
      "|sum(salary)|\n",
      "+-----------+\n",
      "|34000      |\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/functions.py:988: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n",
      "  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|sum(DISTINCT salary)|\n",
      "+--------------------+\n",
      "|20900               |\n",
      "+--------------------+\n",
      "\n",
      "+-----------------+-----------------+---------------+\n",
      "|var_samp(salary) |var_samp(salary) |var_pop(salary)|\n",
      "+-----------------+-----------------+---------------+\n",
      "|586666.6666666666|586666.6666666666|528000.0       |\n",
      "+-----------------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(\"approx_count_distinct: \" + \\\n",
    "#       str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))\n",
    "\n",
    "print(\"avg: \" + str(df.select(avg(\"salary\")).collect()[0][0]))\n",
    "\n",
    "df.select(collect_list(\"salary\")).show(truncate=False)\n",
    "\n",
    "df.select(collect_set(\"salary\")).show(truncate=False)\n",
    "\n",
    "# df2 = df.select(countDistinct(\"department\", \"salary\"))\n",
    "df2.show(truncate=False)\n",
    "print(\"Distinct Count of Department & Salary: \"+str(df2.collect()[0][0]))\n",
    "\n",
    "print(\"count: \"+str(df.select(count(\"salary\")).collect()[0]))\n",
    "df.select(first(\"salary\")).show(truncate=False)\n",
    "df.select(last(\"salary\")).show(truncate=False)\n",
    "df.select(kurtosis(\"salary\")).show(truncate=False)\n",
    "df.select(max(\"salary\")).show(truncate=False)\n",
    "df.select(min(\"salary\")).show(truncate=False)\n",
    "df.select(mean(\"salary\")).show(truncate=False)\n",
    "df.select(skewness(\"salary\")).show(truncate=False)\n",
    "df.select(stddev(\"salary\"), stddev_samp(\"salary\"), \\\n",
    "    stddev_pop(\"salary\")).show(truncate=False)\n",
    "df.select(sum(\"salary\")).show(truncate=False)\n",
    "df.select(sumDistinct(\"salary\")).show(truncate=False)\n",
    "df.select(variance(\"salary\"),var_samp(\"salary\"),var_pop(\"salary\")) \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6140556-f16c-4535-9d1d-36ac4fede2c9",
   "metadata": {},
   "source": [
    "# 15. PySpark SQL - Window functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4111f34-d61d-4b1e-8f95-fc01e04a6ad3",
   "metadata": {},
   "source": [
    "**WINDOW FUNCTIONS USAGE & SYNTAX -\tPYSPARK WINDOW FUNCTIONS DESCRIPTION**\n",
    "* row_number() -\tReturns a sequential number starting from 1 within a window partition\n",
    "* rank() -\tReturns the rank of rows within a window partition, with gaps.\n",
    "* percent_rank() -\tReturns the percentile rank of rows within a window partition.\n",
    "* dense_rank() -\tReturns the rank of rows within a window partition without any gaps. Where as Rank() returns rank with gaps.\n",
    "* ntile(n) -\tReturns the ntile id in a window partition\n",
    "* cume_dist() -\tReturns the cumulative distribution of values within a window partition\n",
    "\n",
    "* lag(e, offset) | lag(columnname, offset) | lag(columnname, offset, defaultvalue) -\t\n",
    "In PySpark, the lag() function retrieves the value of a column from a preceding row within the same window. It enables users to compare values across adjacent rows and perform calculations based on the difference or relationship between consecutive values in a DataFrame.\n",
    "\n",
    "* lead(columnname, offset) | lead(columnname, offset, defaultvalue) - \t\n",
    "The lead() function in PySpark retrieves the value of a column from a succeeding row within the same window. It enables users to access values ahead of the current row and perform comparisons or calculations based on future values in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "da89eb12-d003-463d-bbf8-8aebb6a1ab75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Partitiob by logic common to all functions below\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdd7259-b6ea-4403-8191-2fe9f0970439",
   "metadata": {},
   "source": [
    "### Window Ranking functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "147c8619-8b3c-4984-b529-b658608f2ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|        Maria|   Finance|  3000|         1|\n",
      "|        Scott|   Finance|  3300|         2|\n",
      "|          Jen|   Finance|  3900|         3|\n",
      "|        Kumar| Marketing|  2000|         1|\n",
      "|         Jeff| Marketing|  3000|         2|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|        James|     Sales|  3000|         2|\n",
      "|       Robert|     Sales|  4100|         3|\n",
      "|         Saif|     Sales|  4100|         4|\n",
      "|      Michael|     Sales|  4600|         5|\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# row_number() example\n",
    "df.withColumn(\"row_number\", row_number().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "828bda74-1181-4f3b-b8f1-c5506154d7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|   1|\n",
      "|        Scott|   Finance|  3300|   2|\n",
      "|          Jen|   Finance|  3900|   3|\n",
      "|        Kumar| Marketing|  2000|   1|\n",
      "|         Jeff| Marketing|  3000|   2|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|       Robert|     Sales|  4100|   3|\n",
      "|         Saif|     Sales|  4100|   3|\n",
      "|      Michael|     Sales|  4600|   5|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rank() example\n",
    "df.withColumn(\"rank\",rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a46af7a7-85bc-4e4c-9e82-d3b062076177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------+\n",
      "|employee_name|department|salary|percent_rank|\n",
      "+-------------+----------+------+------------+\n",
      "|        Maria|   Finance|  3000|         0.0|\n",
      "|        Scott|   Finance|  3300|         0.5|\n",
      "|          Jen|   Finance|  3900|         1.0|\n",
      "|        Kumar| Marketing|  2000|         0.0|\n",
      "|         Jeff| Marketing|  3000|         1.0|\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|       Robert|     Sales|  4100|         0.5|\n",
      "|         Saif|     Sales|  4100|         0.5|\n",
      "|      Michael|     Sales|  4600|         1.0|\n",
      "+-------------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# percent_rank() example\n",
    "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "96110f72-f870-4aa9-8c11-73a576195b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----+\n",
      "|employee_name|department|salary|ntile|\n",
      "+-------------+----------+------+-----+\n",
      "|        Maria|   Finance|  3000|    1|\n",
      "|        Scott|   Finance|  3300|    1|\n",
      "|          Jen|   Finance|  3900|    2|\n",
      "|        Kumar| Marketing|  2000|    1|\n",
      "|         Jeff| Marketing|  3000|    2|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|       Robert|     Sales|  4100|    1|\n",
      "|         Saif|     Sales|  4100|    2|\n",
      "|      Michael|     Sales|  4600|    2|\n",
      "+-------------+----------+------+-----+\n",
      "\n",
      "+-------------+----------+------+-----+\n",
      "|employee_name|department|salary|ntile|\n",
      "+-------------+----------+------+-----+\n",
      "|        Maria|   Finance|  3000|    1|\n",
      "|        Scott|   Finance|  3300|    2|\n",
      "|          Jen|   Finance|  3900|    3|\n",
      "|        Kumar| Marketing|  2000|    1|\n",
      "|         Jeff| Marketing|  3000|    2|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|       Robert|     Sales|  4100|    2|\n",
      "|         Saif|     Sales|  4100|    2|\n",
      "|      Michael|     Sales|  4600|    3|\n",
      "+-------------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ntile() example\n",
    "# ntile() window function returns the relative rank of result rows within a window partition. \n",
    "# In the below example we have used 2 as an argument to ntile hence it returns ranking between 2 values (1 and 2)\n",
    "df.withColumn(\"ntile\",ntile(2).over(windowSpec)).show()\n",
    "\n",
    "df.withColumn(\"ntile\",ntile(3).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840c5ee0-de6a-4609-b67d-724c4ddf0dc9",
   "metadata": {},
   "source": [
    "### Window Analytic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9d06a1d5-9a0c-400d-ae9b-b478ac33a16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------------+\n",
      "|employee_name|department|salary|         cume_dist|\n",
      "+-------------+----------+------+------------------+\n",
      "|        Maria|   Finance|  3000|0.3333333333333333|\n",
      "|        Scott|   Finance|  3300|0.6666666666666666|\n",
      "|          Jen|   Finance|  3900|               1.0|\n",
      "|        Kumar| Marketing|  2000|               0.5|\n",
      "|         Jeff| Marketing|  3000|               1.0|\n",
      "|        James|     Sales|  3000|               0.4|\n",
      "|        James|     Sales|  3000|               0.4|\n",
      "|       Robert|     Sales|  4100|               0.8|\n",
      "|         Saif|     Sales|  4100|               0.8|\n",
      "|      Michael|     Sales|  4600|               1.0|\n",
      "+-------------+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cume_dist()\n",
    "# This function computes the cumulative distribution of a value within a window partition. \n",
    "# It calculates the relative rank of a value within the partition.\n",
    "# The result ranges from 0 to 1, where a value of 0 indicates the lowest value in the partition, and 1 indicates the highest.\n",
    "# It’s useful for understanding the distribution of values compared to others within the same partition.\n",
    "\n",
    "df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "914813f2-5938-417e-a4ac-ea8b4fb70eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary| lag|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|NULL|\n",
      "|        Scott|   Finance|  3300|NULL|\n",
      "|          Jen|   Finance|  3900|3000|\n",
      "|        Kumar| Marketing|  2000|NULL|\n",
      "|         Jeff| Marketing|  3000|NULL|\n",
      "|        James|     Sales|  3000|NULL|\n",
      "|        James|     Sales|  3000|NULL|\n",
      "|       Robert|     Sales|  4100|3000|\n",
      "|         Saif|     Sales|  4100|3000|\n",
      "|      Michael|     Sales|  4600|4100|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# log() example\n",
    "df.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e3a4a8-9cd5-4b4b-9e59-6fcb9402b126",
   "metadata": {},
   "source": [
    "### Window Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a561cf93-3290-4901-9954-d5e85b7bb180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+----+----+\n",
      "|department|   avg|  sum| min| max|\n",
      "+----------+------+-----+----+----+\n",
      "|   Finance|3400.0|10200|3000|3900|\n",
      "| Marketing|2500.0| 5000|2000|3000|\n",
      "|     Sales|3760.0|18800|3000|4600|\n",
      "+----------+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8cf6f8-6583-4ab4-84ae-e4b6178a5843",
   "metadata": {},
   "source": [
    "# 16. PySpark SQL - Date and Timestamp Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aef0987a-5d9b-4157-ac32-36104bb38136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|     input|\n",
      "+---+----------+\n",
      "|  1|2020-02-01|\n",
      "|  2|2019-03-01|\n",
      "|  3|2021-03-01|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\n",
    "df=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7ca2cbe8-2467-4911-a9cb-7cbf95c0a5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_date()|\n",
      "+--------------+\n",
      "|    2024-10-12|\n",
      "+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+\n",
      "|date_format|\n",
      "+-----------+\n",
      "| 02-01-2020|\n",
      "| 03-01-2019|\n",
      "| 03-01-2021|\n",
      "+-----------+\n",
      "\n",
      "+----------+\n",
      "|   to_date|\n",
      "+----------+\n",
      "|2020-02-01|\n",
      "|2019-03-01|\n",
      "|2021-03-01|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# current_date()\n",
    "df.select(current_date()).alias(\"curr_date\").show(1)\n",
    "\n",
    "# date_format()\n",
    "df.select(date_format(col(\"input\"), \"MM-dd-yyyy\").alias(\"date_format\")).show()\n",
    "\n",
    "# to_date()\n",
    "df.select(to_date(col(\"input\"),\"yyyy-MM-dd\").alias(\"to_date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e85129a9-0643-4f30-9433-b1b6b3421d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|     input|dafediff|\n",
      "+----------+--------+\n",
      "|2020-02-01|    1715|\n",
      "|2019-03-01|    2052|\n",
      "|2021-03-01|    1321|\n",
      "+----------+--------+\n",
      "\n",
      "+----------+--------------+\n",
      "|     input|months_between|\n",
      "+----------+--------------+\n",
      "|2020-02-01|   56.35483871|\n",
      "|2019-03-01|   67.35483871|\n",
      "|2021-03-01|   43.35483871|\n",
      "+----------+--------------+\n",
      "\n",
      "+----------+-----------+----------+-----------+\n",
      "|     input|Month_Trunc|Month_Year|Month_Trunc|\n",
      "+----------+-----------+----------+-----------+\n",
      "|2020-02-01| 2020-02-01|2020-01-01| 2020-02-01|\n",
      "|2019-03-01| 2019-03-01|2019-01-01| 2019-03-01|\n",
      "|2021-03-01| 2021-03-01|2021-01-01| 2021-03-01|\n",
      "+----------+-----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# datediff()\n",
    "df.select(col(\"input\"), datediff(current_date(),col(\"input\")).alias(\"dafediff\")).show()\n",
    "\n",
    "#months_between()\n",
    "df.select(col(\"input\"), \n",
    "    months_between(current_date(),col(\"input\")).alias(\"months_between\")  \n",
    "  ).show()\n",
    "\n",
    "#trunc()\n",
    "df.select(col(\"input\"), \n",
    "    trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\"), \n",
    "    trunc(col(\"input\"),\"Year\").alias(\"Month_Year\"), \n",
    "    trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\")\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "044d8f3a-1680-4c89-b916-7c34bb13ab43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+\n",
      "|     input|add_months|sub_months|  date_add|  date_sub|\n",
      "+----------+----------+----------+----------+----------+\n",
      "|2020-02-01|2020-05-01|2019-11-01|2020-02-05|2020-01-28|\n",
      "|2019-03-01|2019-06-01|2018-12-01|2019-03-05|2019-02-25|\n",
      "|2021-03-01|2021-06-01|2020-12-01|2021-03-05|2021-02-25|\n",
      "+----------+----------+----------+----------+----------+\n",
      "\n",
      "+----------+----+-----+----------+----------+\n",
      "|     input|year|month|  next_day|weekofyear|\n",
      "+----------+----+-----+----------+----------+\n",
      "|2020-02-01|2020|    2|2020-02-02|         5|\n",
      "|2019-03-01|2019|    3|2019-03-03|         9|\n",
      "|2021-03-01|2021|    3|2021-03-07|         9|\n",
      "+----------+----+-----+----------+----------+\n",
      "\n",
      "+----------+---------+----------+---------+\n",
      "|     input|dayofweek|dayofmonth|dayofyear|\n",
      "+----------+---------+----------+---------+\n",
      "|2020-02-01|        7|         1|       32|\n",
      "|2019-03-01|        6|         1|       60|\n",
      "|2021-03-01|        2|         1|       60|\n",
      "+----------+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#add_months() , date_add(), date_sub()\n",
    "df.select(col(\"input\"), \n",
    "    add_months(col(\"input\"),3).alias(\"add_months\"), \n",
    "    add_months(col(\"input\"),-3).alias(\"sub_months\"), \n",
    "    date_add(col(\"input\"),4).alias(\"date_add\"), \n",
    "    date_sub(col(\"input\"),4).alias(\"date_sub\") \n",
    "  ).show()\n",
    "\n",
    "df.select(col(\"input\"), \n",
    "     year(col(\"input\")).alias(\"year\"), \n",
    "     month(col(\"input\")).alias(\"month\"), \n",
    "     next_day(col(\"input\"),\"Sunday\").alias(\"next_day\"), \n",
    "     weekofyear(col(\"input\")).alias(\"weekofyear\") \n",
    "  ).show()\n",
    "\n",
    "df.select(col(\"input\"),  \n",
    "     dayofweek(col(\"input\")).alias(\"dayofweek\"), \n",
    "     dayofmonth(col(\"input\")).alias(\"dayofmonth\"), \n",
    "     dayofyear(col(\"input\")).alias(\"dayofyear\"), \n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1e058217-00a8-495e-ac61-2a58618941d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n",
      "|id |input                  |\n",
      "+---+-----------------------+\n",
      "|1  |02-01-2020 11 01 19 06 |\n",
      "|2  |03-01-2019 12 01 19 406|\n",
      "|3  |03-01-2021 12 01 19 406|\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[[\"1\",\"02-01-2020 11 01 19 06\"],[\"2\",\"03-01-2019 12 01 19 406\"],[\"3\",\"03-01-2021 12 01 19 406\"]]\n",
    "df2=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "40bef3f4-8e5f-4879-a52e-7e3138f79e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|current_timestamp        |\n",
      "+-------------------------+\n",
      "|2024-10-12 21:23:41.45752|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------------------+-----------------------+\n",
      "|input                  |to_timestamp           |\n",
      "+-----------------------+-----------------------+\n",
      "|02-01-2020 11 01 19 06 |2020-02-01 11:01:19.06 |\n",
      "|03-01-2019 12 01 19 406|2019-03-01 12:01:19.406|\n",
      "|03-01-2021 12 01 19 406|2021-03-01 12:01:19.406|\n",
      "+-----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#current_timestamp()\n",
    "df2.select(current_timestamp().alias(\"current_timestamp\")\n",
    "  ).show(1,truncate=False)\n",
    "\n",
    "#to_timestamp()\n",
    "df2.select(col(\"input\"), \n",
    "    to_timestamp(col(\"input\"), \"MM-dd-yyyy HH mm ss SSS\").alias(\"to_timestamp\") \n",
    "  ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "492d5be1-8d51-4006-ac27-8252edcff812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----+------+------+\n",
      "|input                  |hour|minute|second|\n",
      "+-----------------------+----+------+------+\n",
      "|2020-02-01 11:01:19.06 |11  |1     |19    |\n",
      "|2019-03-01 12:01:19.406|12  |1     |19    |\n",
      "|2021-03-01 12:01:19.406|12  |1     |19    |\n",
      "+-----------------------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hour, minute,second\n",
    "data=[[\"1\",\"2020-02-01 11:01:19.06\"],[\"2\",\"2019-03-01 12:01:19.406\"],[\"3\",\"2021-03-01 12:01:19.406\"]]\n",
    "df3=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "\n",
    "df3.select(col(\"input\"), \n",
    "    hour(col(\"input\")).alias(\"hour\"), \n",
    "    minute(col(\"input\")).alias(\"minute\"),\n",
    "    second(col(\"input\")).alias(\"second\") \n",
    "  ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd99615-08d8-47e8-ac80-e23ecaa34138",
   "metadata": {},
   "source": [
    "# 17. PySpark - Read and Write CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a68a881d-b339-4227-987b-cacd528809b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read CSV File\n",
    "df = spark.read.csv(\"zipcodes.csv\")\n",
    "df.printSchema()\n",
    "\n",
    "# Using format().load()\n",
    "df = spark.read.format(\"csv\").load(\"zipcodes.csv\")\n",
    "#df.printSchema()\n",
    "\n",
    "# Use header record for column names\n",
    "df2 = spark.read.option(\"header\",True) \\\n",
    "     .csv(\"zipcodes.csv\")\n",
    "\n",
    "# Read multiple CSV files\n",
    "#df = spark.read.csv(\"path/file1.csv,path/file2.csv,path/file3.csv\")\n",
    "\n",
    "# Mulitple options at once\n",
    "# Use header record for column names\n",
    "df2 = spark.read.options(header='True',delimiter=',',inferSchema='True') \\\n",
    "    .csv(\"zipcodes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a0f8c2-ae75-4e3e-b07e-a18e2dd7d89f",
   "metadata": {},
   "source": [
    "### Custom Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e14296c5-a5f0-45ad-90fa-ba594ee0da07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+-----------------------+----------------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|_c0         |_c1    |_c2        |_c3                |_c4  |_c5           |_c6  |_c7    |_c8  |_c9  |_c10 |_c11       |_c12   |_c13                   |_c14                        |_c15         |_c16           |_c17               |_c18      |_c19         |\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+-----------------------+----------------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|RecordNumber|Zipcode|ZipCodeType|City               |State|LocationType  |Lat  |Long   |Xaxis|Yaxis|Zaxis|WorldRegion|Country|LocationText           |Location                    |Decommisioned|TaxReturnsFiled|EstimatedPopulation|TotalWages|Notes        |\n",
      "|1           |704    |STANDARD   |PARC PARQUE        |PR   |NOT ACCEPTABLE|17.96|-66.22 |0.38 |-0.87|0.3  |NA         |US     |Parc Parque, PR        |NA-US-PR-PARC PARQUE        |FALSE        |NULL           |NULL               |NULL      |NULL         |\n",
      "|2           |704    |STANDARD   |PASEO COSTA DEL SUR|PR   |NOT ACCEPTABLE|17.96|-66.22 |0.38 |-0.87|0.3  |NA         |US     |Paseo Costa Del Sur, PR|NA-US-PR-PASEO COSTA DEL SUR|FALSE        |NULL           |NULL               |NULL      |NULL         |\n",
      "|10          |709    |STANDARD   |BDA SAN LUIS       |PR   |NOT ACCEPTABLE|18.14|-66.26 |0.38 |-0.86|0.31 |NA         |US     |Bda San Luis, PR       |NA-US-PR-BDA SAN LUIS       |FALSE        |NULL           |NULL               |NULL      |NULL         |\n",
      "|61391       |76166  |UNIQUE     |CINGULAR WIRELESS  |TX   |NOT ACCEPTABLE|32.72|-97.31 |-0.1 |-0.83|0.54 |NA         |US     |Cingular Wireless, TX  |NA-US-TX-CINGULAR WIRELESS  |FALSE        |NULL           |NULL               |NULL      |NULL         |\n",
      "|61392       |76177  |STANDARD   |FORT WORTH         |TX   |PRIMARY       |32.75|-97.33 |-0.1 |-0.83|0.54 |NA         |US     |Fort Worth, TX         |NA-US-TX-FORT WORTH         |FALSE        |2126           |4053               |122396986 |NULL         |\n",
      "|61393       |76177  |STANDARD   |FT WORTH           |TX   |ACCEPTABLE    |32.75|-97.33 |-0.1 |-0.83|0.54 |NA         |US     |Ft Worth, TX           |NA-US-TX-FT WORTH           |FALSE        |2126           |4053               |122396986 |NULL         |\n",
      "|4           |704    |STANDARD   |URB EUGENE RICE    |PR   |NOT ACCEPTABLE|17.96|-66.22 |0.38 |-0.87|0.3  |NA         |US     |Urb Eugene Rice, PR    |NA-US-PR-URB EUGENE RICE    |FALSE        |NULL           |NULL               |NULL      |NULL         |\n",
      "|39827       |85209  |STANDARD   |MESA               |AZ   |PRIMARY       |33.37|-111.64|-0.3 |-0.77|0.55 |NA         |US     |Mesa, AZ               |NA-US-AZ-MESA               |FALSE        |14962          |26883              |563792730 |no NWS data, |\n",
      "|39828       |85210  |STANDARD   |MESA               |AZ   |PRIMARY       |33.38|-111.84|-0.31|-0.77|0.55 |NA         |US     |Mesa, AZ               |NA-US-AZ-MESA               |FALSE        |14374          |25446              |471000465 |NULL         |\n",
      "|49345       |32046  |STANDARD   |HILLIARD           |FL   |PRIMARY       |30.69|-81.92 |0.12 |-0.85|0.51 |NA         |US     |Hilliard, FL           |NA-US-FL-HILLIARD           |FALSE        |3922           |7443               |133112149 |NULL         |\n",
      "|49346       |34445  |PO BOX     |HOLDER             |FL   |PRIMARY       |28.96|-82.41 |0.11 |-0.86|0.48 |NA         |US     |Holder, FL             |NA-US-FL-HOLDER             |FALSE        |NULL           |NULL               |NULL      |NULL         |\n",
      "|49347       |32564  |STANDARD   |HOLT               |FL   |PRIMARY       |30.72|-86.67 |0.04 |-0.85|0.51 |NA         |US     |Holt, FL               |NA-US-FL-HOLT               |FALSE        |1207           |2190               |36395913  |NULL         |\n",
      "|49348       |34487  |PO BOX     |HOMOSASSA          |FL   |PRIMARY       |28.78|-82.61 |0.11 |-0.86|0.48 |NA         |US     |Homosassa, FL          |NA-US-FL-HOMOSASSA          |FALSE        |NULL           |NULL               |NULL      |NULL         |\n",
      "|10          |708    |STANDARD   |BDA SAN LUIS       |PR   |NOT ACCEPTABLE|18.14|-66.26 |0.38 |-0.86|0.31 |NA         |US     |Bda San Luis, PR       |NA-US-PR-BDA SAN LUIS       |FALSE        |NULL           |NULL               |NULL      |NULL         |\n",
      "|3           |704    |STANDARD   |SECT LANAUSSE      |PR   |NOT ACCEPTABLE|17.96|-66.22 |0.38 |-0.87|0.3  |NA         |US     |Sect Lanausse, PR      |NA-US-PR-SECT LANAUSSE      |FALSE        |NULL           |NULL               |NULL      |NULL         |\n",
      "|54354       |36275  |PO BOX     |SPRING GARDEN      |AL   |PRIMARY       |33.97|-85.55 |0.06 |-0.82|0.55 |NA         |US     |Spring Garden, AL      |NA-US-AL-SPRING GARDEN      |FALSE        |NULL           |NULL               |NULL      |NULL         |\n",
      "|54355       |35146  |STANDARD   |SPRINGVILLE        |AL   |PRIMARY       |33.77|-86.47 |0.05 |-0.82|0.55 |NA         |US     |Springville, AL        |NA-US-AL-SPRINGVILLE        |FALSE        |4046           |7845               |172127599 |NULL         |\n",
      "|54356       |35585  |STANDARD   |SPRUCE PINE        |AL   |PRIMARY       |34.37|-87.69 |0.03 |-0.82|0.56 |NA         |US     |Spruce Pine, AL        |NA-US-AL-SPRUCE PINE        |FALSE        |610            |1209               |18525517  |NULL         |\n",
      "|76511       |27007  |STANDARD   |ASH HILL           |NC   |NOT ACCEPTABLE|36.4 |-80.56 |0.13 |-0.79|0.59 |NA         |US     |Ash Hill, NC           |NA-US-NC-ASH HILL           |FALSE        |842            |1666               |28876493  |NULL         |\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+-----------------------+----------------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using custom schema\n",
    "schema = StructType() \\\n",
    "      .add(\"RecordNumber\",IntegerType(),True) \\\n",
    "      .add(\"Zipcode\",IntegerType(),True) \\\n",
    "      .add(\"ZipCodeType\",StringType(),True) \\\n",
    "      .add(\"City\",StringType(),True) \\\n",
    "      .add(\"State\",StringType(),True) \\\n",
    "      .add(\"LocationType\",StringType(),True) \\\n",
    "      .add(\"Lat\",DoubleType(),True) \\\n",
    "      .add(\"Long\",DoubleType(),True) \\\n",
    "      .add(\"Xaxis\",IntegerType(),True) \\\n",
    "      .add(\"Yaxis\",DoubleType(),True) \\\n",
    "      .add(\"Zaxis\",DoubleType(),True) \\\n",
    "      .add(\"WorldRegion\",StringType(),True) \\\n",
    "      .add(\"Country\",StringType(),True) \\\n",
    "      .add(\"LocationText\",StringType(),True) \\\n",
    "      .add(\"Location\",StringType(),True) \\\n",
    "      .add(\"Decommisioned\",BooleanType(),True) \\\n",
    "      .add(\"TaxReturnsFiled\",StringType(),True) \\\n",
    "      .add(\"EstimatedPopulation\",IntegerType(),True) \\\n",
    "      .add(\"TotalWages\",IntegerType(),True) \\\n",
    "      .add(\"Notes\",StringType(),True)\n",
    "      \n",
    "df_with_schema = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(schema) \\\n",
    "      .load(\"zipcodes.csv\")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1ccb63-af0f-4a50-a344-be3a9ef13d6a",
   "metadata": {},
   "source": [
    "### DataFrame Transformations\n",
    "\n",
    "PySpark DataFrame transformations involve applying various operations to manipulate the data within a DataFrame. These transformations include:\n",
    "\n",
    "**Filtering**: Selecting rows from the DataFrame based on specified conditions.  \n",
    "**Selecting Columns**: Extracting specific columns from the DataFrame.  \n",
    "**Adding Columns**: Creating new columns by performing computations or transformations on existing columns.  \n",
    "**Dropping Columns**: Removing unnecessary columns from the DataFrame.  \n",
    "**Grouping and Aggregating**: Grouping rows based on certain criteria and computing aggregate statistics, such as sum, average, count, etc., within each group.    \n",
    "**Sorting**: Arranging the rows of the DataFrame in a specified order based on column values.  \n",
    "**Joining**: Combining two DataFrames based on a common key or condition.  \n",
    "**Union**: Concatenating two DataFrames vertically, adding rows from one DataFrame to another.  \n",
    "**Pivoting and Melting**: Reshaping the DataFrame from long to wide format (pivoting) or from wide to long format (melting).   \n",
    "**Window Functions**: Performing calculations over a sliding window of rows, such as computing moving averages or ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2faca60-1771-480f-86b9-47870587c144",
   "metadata": {},
   "source": [
    "### Write options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "43afb84b-dd57-4547-872f-99c21c0213e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode('overwrite').option(\"header\",True).csv(\"write-zipcodes-csv\")\n",
    "\n",
    "# You can also use this\n",
    "#df2.write.format(\"csv\").mode('overwrite').save(\"/tmp/spark_output/zipcodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e8147-a56e-40b3-a1fa-ddd9facff915",
   "metadata": {},
   "source": [
    "**header** : Specifies whether to include a header row with column names in the CSV file. Example: option(\"header\", \"true\").  \n",
    "**delimiter** : Specifies the delimiter to use between fields in the CSV file. Example: option(\"delimiter\", \",\").  \n",
    "**quote** : Specifies the character used for quoting fields in the CSV file. Example: option(\"quote\", \"\\\"\").  \n",
    "**escape** : Specifies the escape character used in the CSV file. Example: option(\"escape\", \"\\\\\").  \n",
    "**nullValue** : Specifies the string to represent null values in the CSV file. Example: option(\"nullValue\", \"NA\").  \n",
    "**dateFormat** : Specifies the date format to use for date columns. Example: option(\"dateFormat\", \"yyyy-MM-dd\").  \n",
    "**mode** : Specifies the write mode for the output. Options include “overwrite”, “append”, “ignore”, and “error”. Example: option(\"mode\", \"overwrite\").  \n",
    "**compression** : Specifies the compression codec to use for the output file. Example: option(\"compression\", \"gzip\").  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085df120-4474-43e9-bd08-acfd40531a86",
   "metadata": {},
   "source": [
    "# 18. PySpark - Read and Write Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "17762164-5937-4fe2-b4c0-282ae8afe964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|dob  |gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|James    |          |Smith   |36636|M     |3000  |\n",
      "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|dob  |gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|  dob|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|  Robert |          |Williams|42114|     M|  4000|\n",
      "|   Maria |      Anne|   Jones|39192|     F|  4000|\n",
      "| Michael |      Rose|        |40288|     M|  4000|\n",
      "|   James |          |   Smith|36636|     M|  3000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n",
      "+---------+----------+--------+-----+------+\n",
      "|firstname|middlename|lastname|dob  |salary|\n",
      "+---------+----------+--------+-----+------+\n",
      "|Robert   |          |Williams|42114|4000  |\n",
      "|Michael  |Rose      |        |40288|4000  |\n",
      "|James    |          |Smith   |36636|3000  |\n",
      "+---------+----------+--------+-----+------+\n",
      "\n",
      "+---------+----------+--------+-----+------+\n",
      "|firstname|middlename|lastname|  dob|salary|\n",
      "+---------+----------+--------+-----+------+\n",
      "|   Maria |      Anne|   Jones|39192|  4000|\n",
      "|      Jen|      Mary|   Brown|     |    -1|\n",
      "+---------+----------+--------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data =[(\"James \",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "              (\"Michael \",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "              (\"Robert \",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "              (\"Maria \",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "              (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)]\n",
    "columns=[\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "df.write.mode(\"overwrite\").parquet(\"people.parquet\")\n",
    "parDF1=spark.read.parquet(\"people.parquet\")\n",
    "spark.sql(\"drop view ParquetTable\")\n",
    "parDF1.createOrReplaceTempView(\"parquetTable\")\n",
    "parDF1.printSchema()\n",
    "parDF1.show(truncate=False)\n",
    "\n",
    "parkSQL = spark.sql(\"select * from ParquetTable where salary >= 4000 \")\n",
    "parkSQL.show(truncate=False)\n",
    "\n",
    "spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW PERSON USING parquet OPTIONS (path \\\"people.parquet\\\")\")\n",
    "spark.sql(\"SELECT * FROM PERSON\").show()\n",
    "\n",
    "df.write.partitionBy(\"gender\",\"salary\").mode(\"overwrite\").parquet(\"people2.parquet\")\n",
    "\n",
    "parDF2=spark.read.parquet(\"people2.parquet/gender=M\")\n",
    "parDF2.show(truncate=False)\n",
    "\n",
    "spark.sql(\"CREATE OR REPLACE  TEMPORARY VIEW PERSON2 USING parquet OPTIONS (path \\\"people2.parquet/gender=F\\\")\")\n",
    "spark.sql(\"SELECT * FROM PERSON2\" ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a1c82f-7630-433e-b29a-5d4f96c9d532",
   "metadata": {},
   "source": [
    "# 18. Read and Write JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "155ab19a-572b-4c53-9671-6b5c53477fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Decommisioned: boolean (nullable = true)\n",
      " |-- EstimatedPopulation: long (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      " |-- RecordNumber: long (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- TaxReturnsFiled: long (nullable = true)\n",
      " |-- TotalWages: long (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Xaxis: double (nullable = true)\n",
      " |-- Yaxis: double (nullable = true)\n",
      " |-- Zaxis: double (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- Zipcode: long (nullable = true)\n",
      "\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "|               City|Country|Decommisioned|EstimatedPopulation|  Lat|            Location|        LocationText|  LocationType|   Long|        Notes|RecordNumber|State|TaxReturnsFiled|TotalWages|WorldRegion|Xaxis|Yaxis|Zaxis|ZipCodeType|Zipcode|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "|        PARC PARQUE|     US|        false|               NULL|17.96|NA-US-PR-PARC PARQUE|     Parc Parque, PR|NOT ACCEPTABLE| -66.22|         NULL|           1|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|PASEO COSTA DEL SUR|     US|        false|               NULL|17.96|NA-US-PR-PASEO CO...|Paseo Costa Del S...|NOT ACCEPTABLE| -66.22|         NULL|           2|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|       BDA SAN LUIS|     US|        false|               NULL|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE| -66.26|         NULL|          10|   PR|           NULL|      NULL|         NA| 0.38|-0.86| 0.31|   STANDARD|    709|\n",
      "|  CINGULAR WIRELESS|     US|        false|               NULL|32.72|NA-US-TX-CINGULAR...|Cingular Wireless...|NOT ACCEPTABLE| -97.31|         NULL|       61391|   TX|           NULL|      NULL|         NA| -0.1|-0.83| 0.54|     UNIQUE|  76166|\n",
      "|         FORT WORTH|     US|        false|               4053|32.75| NA-US-TX-FORT WORTH|      Fort Worth, TX|       PRIMARY| -97.33|         NULL|       61392|   TX|           2126| 122396986|         NA| -0.1|-0.83| 0.54|   STANDARD|  76177|\n",
      "|           FT WORTH|     US|        false|               4053|32.75|   NA-US-TX-FT WORTH|        Ft Worth, TX|    ACCEPTABLE| -97.33|         NULL|       61393|   TX|           2126| 122396986|         NA| -0.1|-0.83| 0.54|   STANDARD|  76177|\n",
      "|    URB EUGENE RICE|     US|        false|               NULL|17.96|NA-US-PR-URB EUGE...| Urb Eugene Rice, PR|NOT ACCEPTABLE| -66.22|         NULL|           4|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|               MESA|     US|        false|              26883|33.37|       NA-US-AZ-MESA|            Mesa, AZ|       PRIMARY|-111.64|no NWS data, |       39827|   AZ|          14962| 563792730|         NA| -0.3|-0.77| 0.55|   STANDARD|  85209|\n",
      "|               MESA|     US|        false|              25446|33.38|       NA-US-AZ-MESA|            Mesa, AZ|       PRIMARY|-111.84|         NULL|       39828|   AZ|          14374| 471000465|         NA|-0.31|-0.77| 0.55|   STANDARD|  85210|\n",
      "|           HILLIARD|     US|        false|               7443|30.69|   NA-US-FL-HILLIARD|        Hilliard, FL|       PRIMARY| -81.92|         NULL|       49345|   FL|           3922| 133112149|         NA| 0.12|-0.85| 0.51|   STANDARD|  32046|\n",
      "|             HOLDER|     US|        false|               NULL|28.96|     NA-US-FL-HOLDER|          Holder, FL|       PRIMARY| -82.41|         NULL|       49346|   FL|           NULL|      NULL|         NA| 0.11|-0.86| 0.48|     PO BOX|  34445|\n",
      "|               HOLT|     US|        false|               2190|30.72|       NA-US-FL-HOLT|            Holt, FL|       PRIMARY| -86.67|         NULL|       49347|   FL|           1207|  36395913|         NA| 0.04|-0.85| 0.51|   STANDARD|  32564|\n",
      "|          HOMOSASSA|     US|        false|               NULL|28.78|  NA-US-FL-HOMOSASSA|       Homosassa, FL|       PRIMARY| -82.61|         NULL|       49348|   FL|           NULL|      NULL|         NA| 0.11|-0.86| 0.48|     PO BOX|  34487|\n",
      "|       BDA SAN LUIS|     US|        false|               NULL|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE| -66.26|         NULL|          10|   PR|           NULL|      NULL|         NA| 0.38|-0.86| 0.31|   STANDARD|    708|\n",
      "|      SECT LANAUSSE|     US|        false|               NULL|17.96|NA-US-PR-SECT LAN...|   Sect Lanausse, PR|NOT ACCEPTABLE| -66.22|         NULL|           3|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|      SPRING GARDEN|     US|        false|               NULL|33.97|NA-US-AL-SPRING G...|   Spring Garden, AL|       PRIMARY| -85.55|         NULL|       54354|   AL|           NULL|      NULL|         NA| 0.06|-0.82| 0.55|     PO BOX|  36275|\n",
      "|        SPRINGVILLE|     US|        false|               7845|33.77|NA-US-AL-SPRINGVILLE|     Springville, AL|       PRIMARY| -86.47|         NULL|       54355|   AL|           4046| 172127599|         NA| 0.05|-0.82| 0.55|   STANDARD|  35146|\n",
      "|        SPRUCE PINE|     US|        false|               1209|34.37|NA-US-AL-SPRUCE PINE|     Spruce Pine, AL|       PRIMARY| -87.69|         NULL|       54356|   AL|            610|  18525517|         NA| 0.03|-0.82| 0.56|   STANDARD|  35585|\n",
      "|           ASH HILL|     US|        false|               1666| 36.4|   NA-US-NC-ASH HILL|        Ash Hill, NC|NOT ACCEPTABLE| -80.56|         NULL|       76511|   NC|            842|  28876493|         NA| 0.13|-0.79| 0.59|   STANDARD|  27007|\n",
      "|           ASHEBORO|     US|        false|              15228|35.71|   NA-US-NC-ASHEBORO|        Asheboro, NC|       PRIMARY| -79.81|         NULL|       76512|   NC|           8355| 215474318|         NA| 0.14|-0.79| 0.58|   STANDARD|  27203|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------------+------------+-----+-----------+-------+\n",
      "|               City|RecordNumber|State|ZipCodeType|Zipcode|\n",
      "+-------------------+------------+-----+-----------+-------+\n",
      "|PASEO COSTA DEL SUR|           2|   PR|   STANDARD|    704|\n",
      "|       BDA SAN LUIS|          10|   PR|   STANDARD|    709|\n",
      "+-------------------+------------+-----+-----------+-------+\n",
      "\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+---------------+\n",
      "|               City|Country|Decommisioned|EstimatedPopulation|  Lat|            Location|        LocationText|  LocationType|   Long|        Notes|RecordNumber|State|TaxReturnsFiled|TotalWages|WorldRegion|Xaxis|Yaxis|Zaxis|ZipCodeType|Zipcode|_corrupt_record|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+---------------+\n",
      "|        PARC PARQUE|     US|        false|               NULL|17.96|NA-US-PR-PARC PARQUE|     Parc Parque, PR|NOT ACCEPTABLE| -66.22|         NULL|           1|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|           NULL|\n",
      "|PASEO COSTA DEL SUR|     US|        false|               NULL|17.96|NA-US-PR-PASEO CO...|Paseo Costa Del S...|NOT ACCEPTABLE| -66.22|         NULL|           2|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|           NULL|\n",
      "|       BDA SAN LUIS|     US|        false|               NULL|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE| -66.26|         NULL|          10|   PR|           NULL|      NULL|         NA| 0.38|-0.86| 0.31|   STANDARD|    709|           NULL|\n",
      "|  CINGULAR WIRELESS|     US|        false|               NULL|32.72|NA-US-TX-CINGULAR...|Cingular Wireless...|NOT ACCEPTABLE| -97.31|         NULL|       61391|   TX|           NULL|      NULL|         NA| -0.1|-0.83| 0.54|     UNIQUE|  76166|           NULL|\n",
      "|         FORT WORTH|     US|        false|               4053|32.75| NA-US-TX-FORT WORTH|      Fort Worth, TX|       PRIMARY| -97.33|         NULL|       61392|   TX|           2126| 122396986|         NA| -0.1|-0.83| 0.54|   STANDARD|  76177|           NULL|\n",
      "|           FT WORTH|     US|        false|               4053|32.75|   NA-US-TX-FT WORTH|        Ft Worth, TX|    ACCEPTABLE| -97.33|         NULL|       61393|   TX|           2126| 122396986|         NA| -0.1|-0.83| 0.54|   STANDARD|  76177|           NULL|\n",
      "|    URB EUGENE RICE|     US|        false|               NULL|17.96|NA-US-PR-URB EUGE...| Urb Eugene Rice, PR|NOT ACCEPTABLE| -66.22|         NULL|           4|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|           NULL|\n",
      "|               MESA|     US|        false|              26883|33.37|       NA-US-AZ-MESA|            Mesa, AZ|       PRIMARY|-111.64|no NWS data, |       39827|   AZ|          14962| 563792730|         NA| -0.3|-0.77| 0.55|   STANDARD|  85209|           NULL|\n",
      "|               MESA|     US|        false|              25446|33.38|       NA-US-AZ-MESA|            Mesa, AZ|       PRIMARY|-111.84|         NULL|       39828|   AZ|          14374| 471000465|         NA|-0.31|-0.77| 0.55|   STANDARD|  85210|           NULL|\n",
      "|           HILLIARD|     US|        false|               7443|30.69|   NA-US-FL-HILLIARD|        Hilliard, FL|       PRIMARY| -81.92|         NULL|       49345|   FL|           3922| 133112149|         NA| 0.12|-0.85| 0.51|   STANDARD|  32046|           NULL|\n",
      "|             HOLDER|     US|        false|               NULL|28.96|     NA-US-FL-HOLDER|          Holder, FL|       PRIMARY| -82.41|         NULL|       49346|   FL|           NULL|      NULL|         NA| 0.11|-0.86| 0.48|     PO BOX|  34445|           NULL|\n",
      "|               HOLT|     US|        false|               2190|30.72|       NA-US-FL-HOLT|            Holt, FL|       PRIMARY| -86.67|         NULL|       49347|   FL|           1207|  36395913|         NA| 0.04|-0.85| 0.51|   STANDARD|  32564|           NULL|\n",
      "|          HOMOSASSA|     US|        false|               NULL|28.78|  NA-US-FL-HOMOSASSA|       Homosassa, FL|       PRIMARY| -82.61|         NULL|       49348|   FL|           NULL|      NULL|         NA| 0.11|-0.86| 0.48|     PO BOX|  34487|           NULL|\n",
      "|       BDA SAN LUIS|     US|        false|               NULL|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE| -66.26|         NULL|          10|   PR|           NULL|      NULL|         NA| 0.38|-0.86| 0.31|   STANDARD|    708|           NULL|\n",
      "|      SECT LANAUSSE|     US|        false|               NULL|17.96|NA-US-PR-SECT LAN...|   Sect Lanausse, PR|NOT ACCEPTABLE| -66.22|         NULL|           3|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|           NULL|\n",
      "|      SPRING GARDEN|     US|        false|               NULL|33.97|NA-US-AL-SPRING G...|   Spring Garden, AL|       PRIMARY| -85.55|         NULL|       54354|   AL|           NULL|      NULL|         NA| 0.06|-0.82| 0.55|     PO BOX|  36275|           NULL|\n",
      "|        SPRINGVILLE|     US|        false|               7845|33.77|NA-US-AL-SPRINGVILLE|     Springville, AL|       PRIMARY| -86.47|         NULL|       54355|   AL|           4046| 172127599|         NA| 0.05|-0.82| 0.55|   STANDARD|  35146|           NULL|\n",
      "|        SPRUCE PINE|     US|        false|               1209|34.37|NA-US-AL-SPRUCE PINE|     Spruce Pine, AL|       PRIMARY| -87.69|         NULL|       54356|   AL|            610|  18525517|         NA| 0.03|-0.82| 0.56|   STANDARD|  35585|           NULL|\n",
      "|           ASH HILL|     US|        false|               1666| 36.4|   NA-US-NC-ASH HILL|        Ash Hill, NC|NOT ACCEPTABLE| -80.56|         NULL|       76511|   NC|            842|  28876493|         NA| 0.13|-0.79| 0.59|   STANDARD|  27007|           NULL|\n",
      "|           ASHEBORO|     US|        false|              15228|35.71|   NA-US-NC-ASHEBORO|        Asheboro, NC|       PRIMARY| -79.81|         NULL|       76512|   NC|           8355| 215474318|         NA| 0.14|-0.79| 0.58|   STANDARD|  27203|           NULL|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- RecordNumber: integer (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Xaxis: integer (nullable = true)\n",
      " |-- Yaxis: double (nullable = true)\n",
      " |-- Zaxis: double (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Decommisioned: boolean (nullable = true)\n",
      " |-- TaxReturnsFiled: string (nullable = true)\n",
      " |-- EstimatedPopulation: integer (nullable = true)\n",
      " |-- TotalWages: integer (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      "\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|RecordNumber|Zipcode|ZipCodeType|               City|State|  LocationType|  Lat|   Long|Xaxis|Yaxis|Zaxis|WorldRegion|Country|        LocationText|            Location|Decommisioned|TaxReturnsFiled|EstimatedPopulation|TotalWages|        Notes|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|           1|    704|   STANDARD|        PARC PARQUE|   PR|NOT ACCEPTABLE|17.96| -66.22| NULL|-0.87|  0.3|         NA|     US|     Parc Parque, PR|NA-US-PR-PARC PARQUE|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|           2|    704|   STANDARD|PASEO COSTA DEL SUR|   PR|NOT ACCEPTABLE|17.96| -66.22| NULL|-0.87|  0.3|         NA|     US|Paseo Costa Del S...|NA-US-PR-PASEO CO...|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|          10|    709|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14| -66.26| NULL|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|       61391|  76166|     UNIQUE|  CINGULAR WIRELESS|   TX|NOT ACCEPTABLE|32.72| -97.31| NULL|-0.83| 0.54|         NA|     US|Cingular Wireless...|NA-US-TX-CINGULAR...|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|       61392|  76177|   STANDARD|         FORT WORTH|   TX|       PRIMARY|32.75| -97.33| NULL|-0.83| 0.54|         NA|     US|      Fort Worth, TX| NA-US-TX-FORT WORTH|        false|           2126|               4053| 122396986|         NULL|\n",
      "|       61393|  76177|   STANDARD|           FT WORTH|   TX|    ACCEPTABLE|32.75| -97.33| NULL|-0.83| 0.54|         NA|     US|        Ft Worth, TX|   NA-US-TX-FT WORTH|        false|           2126|               4053| 122396986|         NULL|\n",
      "|           4|    704|   STANDARD|    URB EUGENE RICE|   PR|NOT ACCEPTABLE|17.96| -66.22| NULL|-0.87|  0.3|         NA|     US| Urb Eugene Rice, PR|NA-US-PR-URB EUGE...|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|       39827|  85209|   STANDARD|               MESA|   AZ|       PRIMARY|33.37|-111.64| NULL|-0.77| 0.55|         NA|     US|            Mesa, AZ|       NA-US-AZ-MESA|        false|          14962|              26883| 563792730|no NWS data, |\n",
      "|       39828|  85210|   STANDARD|               MESA|   AZ|       PRIMARY|33.38|-111.84| NULL|-0.77| 0.55|         NA|     US|            Mesa, AZ|       NA-US-AZ-MESA|        false|          14374|              25446| 471000465|         NULL|\n",
      "|       49345|  32046|   STANDARD|           HILLIARD|   FL|       PRIMARY|30.69| -81.92| NULL|-0.85| 0.51|         NA|     US|        Hilliard, FL|   NA-US-FL-HILLIARD|        false|           3922|               7443| 133112149|         NULL|\n",
      "|       49346|  34445|     PO BOX|             HOLDER|   FL|       PRIMARY|28.96| -82.41| NULL|-0.86| 0.48|         NA|     US|          Holder, FL|     NA-US-FL-HOLDER|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|       49347|  32564|   STANDARD|               HOLT|   FL|       PRIMARY|30.72| -86.67| NULL|-0.85| 0.51|         NA|     US|            Holt, FL|       NA-US-FL-HOLT|        false|           1207|               2190|  36395913|         NULL|\n",
      "|       49348|  34487|     PO BOX|          HOMOSASSA|   FL|       PRIMARY|28.78| -82.61| NULL|-0.86| 0.48|         NA|     US|       Homosassa, FL|  NA-US-FL-HOMOSASSA|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|          10|    708|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14| -66.26| NULL|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|           3|    704|   STANDARD|      SECT LANAUSSE|   PR|NOT ACCEPTABLE|17.96| -66.22| NULL|-0.87|  0.3|         NA|     US|   Sect Lanausse, PR|NA-US-PR-SECT LAN...|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|       54354|  36275|     PO BOX|      SPRING GARDEN|   AL|       PRIMARY|33.97| -85.55| NULL|-0.82| 0.55|         NA|     US|   Spring Garden, AL|NA-US-AL-SPRING G...|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|       54355|  35146|   STANDARD|        SPRINGVILLE|   AL|       PRIMARY|33.77| -86.47| NULL|-0.82| 0.55|         NA|     US|     Springville, AL|NA-US-AL-SPRINGVILLE|        false|           4046|               7845| 172127599|         NULL|\n",
      "|       54356|  35585|   STANDARD|        SPRUCE PINE|   AL|       PRIMARY|34.37| -87.69| NULL|-0.82| 0.56|         NA|     US|     Spruce Pine, AL|NA-US-AL-SPRUCE PINE|        false|            610|               1209|  18525517|         NULL|\n",
      "|       76511|  27007|   STANDARD|           ASH HILL|   NC|NOT ACCEPTABLE| 36.4| -80.56| NULL|-0.79| 0.59|         NA|     US|        Ash Hill, NC|   NA-US-NC-ASH HILL|        false|            842|               1666|  28876493|         NULL|\n",
      "|       76512|  27203|   STANDARD|           ASHEBORO|   NC|       PRIMARY|35.71| -79.81| NULL|-0.79| 0.58|         NA|     US|        Asheboro, NC|   NA-US-NC-ASHEBORO|        false|           8355|              15228| 215474318|         NULL|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "|               City|Country|Decommisioned|EstimatedPopulation|  Lat|            Location|        LocationText|  LocationType|   Long|        Notes|RecordNumber|State|TaxReturnsFiled|TotalWages|WorldRegion|Xaxis|Yaxis|Zaxis|ZipCodeType|Zipcode|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "|        PARC PARQUE|     US|        false|               NULL|17.96|NA-US-PR-PARC PARQUE|     Parc Parque, PR|NOT ACCEPTABLE| -66.22|         NULL|           1|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|PASEO COSTA DEL SUR|     US|        false|               NULL|17.96|NA-US-PR-PASEO CO...|Paseo Costa Del S...|NOT ACCEPTABLE| -66.22|         NULL|           2|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|       BDA SAN LUIS|     US|        false|               NULL|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE| -66.26|         NULL|          10|   PR|           NULL|      NULL|         NA| 0.38|-0.86| 0.31|   STANDARD|    709|\n",
      "|  CINGULAR WIRELESS|     US|        false|               NULL|32.72|NA-US-TX-CINGULAR...|Cingular Wireless...|NOT ACCEPTABLE| -97.31|         NULL|       61391|   TX|           NULL|      NULL|         NA| -0.1|-0.83| 0.54|     UNIQUE|  76166|\n",
      "|         FORT WORTH|     US|        false|               4053|32.75| NA-US-TX-FORT WORTH|      Fort Worth, TX|       PRIMARY| -97.33|         NULL|       61392|   TX|           2126| 122396986|         NA| -0.1|-0.83| 0.54|   STANDARD|  76177|\n",
      "|           FT WORTH|     US|        false|               4053|32.75|   NA-US-TX-FT WORTH|        Ft Worth, TX|    ACCEPTABLE| -97.33|         NULL|       61393|   TX|           2126| 122396986|         NA| -0.1|-0.83| 0.54|   STANDARD|  76177|\n",
      "|    URB EUGENE RICE|     US|        false|               NULL|17.96|NA-US-PR-URB EUGE...| Urb Eugene Rice, PR|NOT ACCEPTABLE| -66.22|         NULL|           4|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|               MESA|     US|        false|              26883|33.37|       NA-US-AZ-MESA|            Mesa, AZ|       PRIMARY|-111.64|no NWS data, |       39827|   AZ|          14962| 563792730|         NA| -0.3|-0.77| 0.55|   STANDARD|  85209|\n",
      "|               MESA|     US|        false|              25446|33.38|       NA-US-AZ-MESA|            Mesa, AZ|       PRIMARY|-111.84|         NULL|       39828|   AZ|          14374| 471000465|         NA|-0.31|-0.77| 0.55|   STANDARD|  85210|\n",
      "|           HILLIARD|     US|        false|               7443|30.69|   NA-US-FL-HILLIARD|        Hilliard, FL|       PRIMARY| -81.92|         NULL|       49345|   FL|           3922| 133112149|         NA| 0.12|-0.85| 0.51|   STANDARD|  32046|\n",
      "|             HOLDER|     US|        false|               NULL|28.96|     NA-US-FL-HOLDER|          Holder, FL|       PRIMARY| -82.41|         NULL|       49346|   FL|           NULL|      NULL|         NA| 0.11|-0.86| 0.48|     PO BOX|  34445|\n",
      "|               HOLT|     US|        false|               2190|30.72|       NA-US-FL-HOLT|            Holt, FL|       PRIMARY| -86.67|         NULL|       49347|   FL|           1207|  36395913|         NA| 0.04|-0.85| 0.51|   STANDARD|  32564|\n",
      "|          HOMOSASSA|     US|        false|               NULL|28.78|  NA-US-FL-HOMOSASSA|       Homosassa, FL|       PRIMARY| -82.61|         NULL|       49348|   FL|           NULL|      NULL|         NA| 0.11|-0.86| 0.48|     PO BOX|  34487|\n",
      "|       BDA SAN LUIS|     US|        false|               NULL|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE| -66.26|         NULL|          10|   PR|           NULL|      NULL|         NA| 0.38|-0.86| 0.31|   STANDARD|    708|\n",
      "|      SECT LANAUSSE|     US|        false|               NULL|17.96|NA-US-PR-SECT LAN...|   Sect Lanausse, PR|NOT ACCEPTABLE| -66.22|         NULL|           3|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|      SPRING GARDEN|     US|        false|               NULL|33.97|NA-US-AL-SPRING G...|   Spring Garden, AL|       PRIMARY| -85.55|         NULL|       54354|   AL|           NULL|      NULL|         NA| 0.06|-0.82| 0.55|     PO BOX|  36275|\n",
      "|        SPRINGVILLE|     US|        false|               7845|33.77|NA-US-AL-SPRINGVILLE|     Springville, AL|       PRIMARY| -86.47|         NULL|       54355|   AL|           4046| 172127599|         NA| 0.05|-0.82| 0.55|   STANDARD|  35146|\n",
      "|        SPRUCE PINE|     US|        false|               1209|34.37|NA-US-AL-SPRUCE PINE|     Spruce Pine, AL|       PRIMARY| -87.69|         NULL|       54356|   AL|            610|  18525517|         NA| 0.03|-0.82| 0.56|   STANDARD|  35585|\n",
      "|           ASH HILL|     US|        false|               1666| 36.4|   NA-US-NC-ASH HILL|        Ash Hill, NC|NOT ACCEPTABLE| -80.56|         NULL|       76511|   NC|            842|  28876493|         NA| 0.13|-0.79| 0.59|   STANDARD|  27007|\n",
      "|           ASHEBORO|     US|        false|              15228|35.71|   NA-US-NC-ASHEBORO|        Asheboro, NC|       PRIMARY| -79.81|         NULL|       76512|   NC|           8355| 215474318|         NA| 0.14|-0.79| 0.58|   STANDARD|  27203|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read JSON file into dataframe    \n",
    "df = spark.read.json(\"zipcodes.json\")\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "# Read multiline json file\n",
    "multiline_df = spark.read.option(\"multiline\",\"true\") \\\n",
    "      .json(\"multiline-zipcode.json\")\n",
    "multiline_df.show()\n",
    "\n",
    "#Read multiple files\n",
    "# df2 = spark.read.json(\n",
    "#     ['resources/zipcode2.json','resources/zipcode1.json'])\n",
    "# df2.show()    \n",
    "\n",
    "#Read All JSON files from a directory\n",
    "df3 = spark.read.json(\"*.json\")\n",
    "df3.show()\n",
    "\n",
    "# Define custom schema\n",
    "schema = StructType([\n",
    "      StructField(\"RecordNumber\",IntegerType(),True),\n",
    "      StructField(\"Zipcode\",IntegerType(),True),\n",
    "      StructField(\"ZipCodeType\",StringType(),True),\n",
    "      StructField(\"City\",StringType(),True),\n",
    "      StructField(\"State\",StringType(),True),\n",
    "      StructField(\"LocationType\",StringType(),True),\n",
    "      StructField(\"Lat\",DoubleType(),True),\n",
    "      StructField(\"Long\",DoubleType(),True),\n",
    "      StructField(\"Xaxis\",IntegerType(),True),\n",
    "      StructField(\"Yaxis\",DoubleType(),True),\n",
    "      StructField(\"Zaxis\",DoubleType(),True),\n",
    "      StructField(\"WorldRegion\",StringType(),True),\n",
    "      StructField(\"Country\",StringType(),True),\n",
    "      StructField(\"LocationText\",StringType(),True),\n",
    "      StructField(\"Location\",StringType(),True),\n",
    "      StructField(\"Decommisioned\",BooleanType(),True),\n",
    "      StructField(\"TaxReturnsFiled\",StringType(),True),\n",
    "      StructField(\"EstimatedPopulation\",IntegerType(),True),\n",
    "      StructField(\"TotalWages\",IntegerType(),True),\n",
    "      StructField(\"Notes\",StringType(),True)\n",
    "  ])\n",
    "\n",
    "df_with_schema = spark.read.schema(schema) \\\n",
    "        .json(\"zipcodes.json\")\n",
    "df_with_schema.printSchema()\n",
    "df_with_schema.show()\n",
    "\n",
    "# Create a table from Parquet File\n",
    "spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW zipcode3 USING json OPTIONS\" + \n",
    "      \" (path 'zipcodes.json')\")\n",
    "spark.sql(\"select * from zipcode3\").show()\n",
    "\n",
    "# PySpark write Parquet File\n",
    "df2.write.mode('Overwrite').json(\"write-zipcodes-json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c45fc1-1fbf-45c4-87e2-05d236809431",
   "metadata": {},
   "source": [
    "# 19. PySpark - Built in Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1b8b69-39ec-4f91-8892-e9ba1ed18d29",
   "metadata": {},
   "source": [
    "### when() or otherwise() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "aebfa3ef-92f1-410c-8775-0b3c1675ebad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|   name|gender|salary|\n",
      "+-------+------+------+\n",
      "|  James|     M| 60000|\n",
      "|Michael|     M| 70000|\n",
      "| Robert|  NULL|400000|\n",
      "|  Maria|     F|500000|\n",
      "|    Jen|      |  NULL|\n",
      "+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"M\",60000),(\"Michael\",\"M\",70000),\n",
    "        (\"Robert\",None,400000),(\"Maria\",\"F\",500000),\n",
    "        (\"Jen\",\"\",None)]\n",
    "\n",
    "columns = [\"name\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f81b5069-f869-4465-a1dd-ca790b9c6652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n",
      "|   name|gender|salary|new_gender|\n",
      "+-------+------+------+----------+\n",
      "|  James|     M| 60000|      Male|\n",
      "|Michael|     M| 70000|      Male|\n",
      "| Robert|  NULL|400000|          |\n",
      "|  Maria|     F|500000|    Female|\n",
      "|    Jen|      |  NULL|          |\n",
      "+-------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when() or otherwise() function\n",
    "\n",
    "df2 = df.withColumn(\"new_gender\", when(df.gender == \"M\", \"Male\") \\\n",
    "             .when(df.gender == \"F\",\"Female\") \\\n",
    "             .when(df.gender.isNull(),\"\") \\\n",
    "             .otherwise(df.gender))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6eda7c-0c2f-4c64-89fa-b75b4227521b",
   "metadata": {},
   "source": [
    "### expr() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3ae4e128-a0ec-42b1-b08d-36c16b32ea30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n",
      "|name   |gender|salary|new_gender|\n",
      "+-------+------+------+----------+\n",
      "|James  |M     |60000 |Male      |\n",
      "|Michael|M     |70000 |Male      |\n",
      "|Robert |NULL  |400000|          |\n",
      "|Maria  |F     |500000|Female    |\n",
      "|Jen    |      |NULL  |          |\n",
      "+-------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using Case When on withColumn()\n",
    "df3 = df.withColumn(\"new_gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' \" + \n",
    "               \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n",
    "               \"ELSE gender END\"))\n",
    "df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e92f1b4e-8505-4218-b83a-8afa8cfc28b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n",
      "|   name|gender|salary|new_gender|\n",
      "+-------+------+------+----------+\n",
      "|  James|     M| 60000|      Male|\n",
      "|Michael|     M| 70000|      Male|\n",
      "| Robert|  NULL|400000|          |\n",
      "|  Maria|     F|500000|    Female|\n",
      "|    Jen|      |  NULL|          |\n",
      "+-------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using Case When on select()\n",
    "df4 = df.select(col(\"*\"), expr(\"CASE WHEN gender = 'M' THEN 'Male' \" +\n",
    "           \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n",
    "           \"ELSE gender END\").alias(\"new_gender\"))\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4ea86799-70c0-441d-b3b3-2116691e2e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+\n",
      "|      date|increment|  inc_date|\n",
      "+----------+---------+----------+\n",
      "|2019-01-23|        1|2019-02-23|\n",
      "|2019-06-24|        2|2019-08-24|\n",
      "|2019-09-20|        3|2019-12-20|\n",
      "+----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)] \n",
    "df=spark.createDataFrame(data).toDF(\"date\",\"increment\") \n",
    "\n",
    "#Add Month value from another column\n",
    "df.select(df.date,df.increment,\n",
    "     expr(\"add_months(date,increment)\")\n",
    "  .alias(\"inc_date\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d06cddf-1ee0-4063-919d-8a418470d87a",
   "metadata": {},
   "source": [
    "### split() function\n",
    "\n",
    "**Syntax:**  \n",
    "pyspark.sql.functions.split(str, pattern, limit=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "76b43efa-3896-463f-9895-ae57b938bbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- dob_year: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+--------------------+--------+------+------+\n",
      "|                name|dob_year|gender|salary|\n",
      "+--------------------+--------+------+------+\n",
      "|     James, A, Smith|    2018|     M|  3000|\n",
      "|Michael, Rose, Jones|    2010|     M|  4000|\n",
      "|   Robert,K,Williams|    2010|     M|  4000|\n",
      "|    Maria,Anne,Jones|    2005|     F|  4000|\n",
      "|      Jen,Mary,Brown|    2010|      |    -1|\n",
      "+--------------------+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "data = [(\"James, A, Smith\",\"2018\",\"M\",3000),\n",
    "            (\"Michael, Rose, Jones\",\"2010\",\"M\",4000),\n",
    "            (\"Robert,K,Williams\",\"2010\",\"M\",4000),\n",
    "            (\"Maria,Anne,Jones\",\"2005\",\"F\",4000),\n",
    "            (\"Jen,Mary,Brown\",\"2010\",\"\",-1)\n",
    "            ]\n",
    "\n",
    "columns=[\"name\",\"dob_year\",\"gender\",\"salary\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df=spark.createDataFrame(data,columns)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d1b9c638-da9b-4b9b-b3cd-ba7a4d64363b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- split(name, ,, -1): array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n",
      "+------------------------+\n",
      "|split(name, ,, -1)      |\n",
      "+------------------------+\n",
      "|[James,  A,  Smith]     |\n",
      "|[Michael,  Rose,  Jones]|\n",
      "|[Robert, K, Williams]   |\n",
      "|[Maria, Anne, Jones]    |\n",
      "|[Jen, Mary, Brown]      |\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.select(split(col(\"name\"),\",\")).alias(\"NameArray\")\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "21e07ca3-f375-419a-82c9-ddcd4ffc46ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------+------+----------------------+\n",
      "|name                |dob_year|gender|salary|name_array            |\n",
      "+--------------------+--------+------+------+----------------------+\n",
      "|James, A, Smith     |2018    |M     |3000  |[James, A, Smith]     |\n",
      "|Michael, Rose, Jones|2010    |M     |4000  |[Michael, Rose, Jones]|\n",
      "|Robert,K,Williams   |2010    |M     |4000  |[Robert, K, Williams] |\n",
      "|Maria,Anne,Jones    |2005    |F     |4000  |[Maria, Anne, Jones]  |\n",
      "|Jen,Mary,Brown      |2010    |      |-1    |[Jen, Mary, Brown]    |\n",
      "+--------------------+--------+------+------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"name_array\",split(col(\"name\"),\",\\s*\"))\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1552d236-07e4-454e-8b83-336ee142b83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|NameArray               |\n",
      "+------------------------+\n",
      "|[James,  A,  Smith]     |\n",
      "|[Michael,  Rose,  Jones]|\n",
      "|[Robert, K, Williams]   |\n",
      "|[Maria, Anne, Jones]    |\n",
      "|[Jen, Mary, Brown]      |\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run SQL query\n",
    "df.createOrReplaceTempView(\"PERSON\")\n",
    "spark.sql(\"select SPLIT(name,',') as NameArray from PERSON\") \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319b81bd-5b28-4aae-a8bb-3122c53abdb4",
   "metadata": {},
   "source": [
    "### substring() function\n",
    "\n",
    "**Syntax:**  \n",
    "substring(str, pos, len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "58690268-048d-4a29-98fd-6efc8c8830b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n",
      "+---+--------+\n",
      "|id |date    |\n",
      "+---+--------+\n",
      "|1  |20200828|\n",
      "|2  |20180525|\n",
      "+---+--------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      "\n",
      "+---+--------+----+-----+---+\n",
      "|id |date    |year|month|day|\n",
      "+---+--------+----+-----+---+\n",
      "|1  |20200828|2020|08   |28 |\n",
      "|2  |20180525|2018|05   |25 |\n",
      "+---+--------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Sample Data\n",
    "data = [(1,\"20200828\"),(2,\"20180525\")]\n",
    "columns=[\"id\",\"date\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Using substring()\n",
    "df2 = df.withColumn('year', substring('date', 1,4)) \\\n",
    "    .withColumn('month', substring('date', 5,2)) \\\n",
    "    .withColumn('day', substring('date', 7,2))\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c53aa6-4aa5-42b1-8d71-74b7f9b9ae3e",
   "metadata": {},
   "source": [
    "### explode(), array() and array_contains() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9dfbc356-a75b-404c-8a53-553e474af6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- languagesAtWork: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      " |-- previousState: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+---------------+------------+-------------+\n",
      "|            name| languagesAtSchool|languagesAtWork|currentState|previousState|\n",
      "+----------------+------------------+---------------+------------+-------------+\n",
      "|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|          OH|           CA|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|          NY|           NJ|\n",
      "|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|          UT|           NV|\n",
      "+----------------+------------------+---------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n",
    " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n",
    " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")\n",
    "]\n",
    "\n",
    "schema = StructType([ \n",
    "    StructField(\"name\",StringType(),True), \n",
    "    StructField(\"languagesAtSchool\",ArrayType(StringType()),True), \n",
    "    StructField(\"languagesAtWork\",ArrayType(StringType()),True), \n",
    "    StructField(\"currentState\", StringType(), True), \n",
    "    StructField(\"previousState\", StringType(), True)\n",
    "  ])\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d4b6cbad-fa44-492c-928e-17ffb7c8a903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|            name|   col|\n",
      "+----------------+------+\n",
      "|    James,,Smith|  Java|\n",
      "|    James,,Smith| Scala|\n",
      "|    James,,Smith|   C++|\n",
      "|   Michael,Rose,| Spark|\n",
      "|   Michael,Rose,|  Java|\n",
      "|   Michael,Rose,|   C++|\n",
      "|Robert,,Williams|CSharp|\n",
      "|Robert,,Williams|    VB|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explode() function:\n",
    "#Use explode() function to create a new row for each element in the given array column.\n",
    "#There are various PySpark SQL explode functions available to work with Array columns.\n",
    "\n",
    "df.select(df.name,explode(df.languagesAtSchool)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5c0f32ee-eff4-4596-b15e-7670f5b545bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+\n",
      "|            name|  states|\n",
      "+----------------+--------+\n",
      "|    James,,Smith|[OH, CA]|\n",
      "|   Michael,Rose,|[NY, NJ]|\n",
      "|Robert,,Williams|[UT, NV]|\n",
      "+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# array() function:\n",
    "#Use array() function to create a new array column by merging the data from multiple columns. \n",
    "#All input columns must have the same data type.\n",
    "\n",
    "df.select(df.name,array(df.currentState,df.previousState).alias(\"states\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b940c0fe-f684-4678-bc7e-7c4683dd4a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+\n",
      "|            name|array_contains|\n",
      "+----------------+--------------+\n",
      "|    James,,Smith|          true|\n",
      "|   Michael,Rose,|          true|\n",
      "|Robert,,Williams|         false|\n",
      "+----------------+--------------+\n",
      "\n",
      "+----------------+------------------+---------------+------------+-------------+-------------------+\n",
      "|            name| languagesAtSchool|languagesAtWork|currentState|previousState|array_contains_java|\n",
      "+----------------+------------------+---------------+------------+-------------+-------------------+\n",
      "|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|          OH|           CA|               true|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|          NY|           NJ|               true|\n",
      "|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|          UT|           NV|              false|\n",
      "+----------------+------------------+---------------+------------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# array_contains() function:\n",
    "df.select(df.name,array_contains(df.languagesAtSchool, \"Java\").alias(\"array_contains\")).show()\n",
    "\n",
    "# another way of writing it\n",
    "df2 = df.withColumn(\"array_contains_java\",array_contains(df.languagesAtSchool,\"Java\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15037c84-fef5-4d46-a3af-6f93fb55bc1b",
   "metadata": {},
   "source": [
    "### Json Functions:\n",
    "\n",
    "**JSON Functions -\tDescription**  \n",
    "from_json() - \tConverts JSON string into Struct type or Map type.  \n",
    "to_json() - \tConverts MapType or Struct type to JSON string.  \n",
    "json_tuple() - \tExtract the Data from JSON and create them as a new columns.  \n",
    "get_json_object() - \tExtracts JSON element from a JSON string based on json path specified.  \n",
    "schema_of_json() - \tCreate schema string from JSON string.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "20eb3d67-223d-40d2-9640-c95e2ff8f7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------------------------------+\n",
      "|id |value                                                                     |\n",
      "+---+--------------------------------------------------------------------------+\n",
      "|1  |{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n",
      "+---+--------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonString=\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"\n",
    "df = spark.createDataFrame([(1, jsonString)],[\"id\",\"value\"])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a66aedbe-adce-404c-be57-ff0be88fc5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- value: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+---+---------------------------------------------------------------------------+\n",
      "|id |value                                                                      |\n",
      "+---+---------------------------------------------------------------------------+\n",
      "|1  |{Zipcode -> 704, ZipCodeType -> STANDARD, City -> PARC PARQUE, State -> PR}|\n",
      "+---+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from_json() function:\n",
    "# PySpark from_json() function is used to convert JSON string into Struct type or Map type\n",
    "\n",
    "df2 = df.withColumn(\"value\",from_json(df.value,MapType(StringType(),StringType())))\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4592bd31-2c59-4b50-a6f4-2b78e0834855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------------------------------------+\n",
      "|id |value                                                                       |\n",
      "+---+----------------------------------------------------------------------------+\n",
      "|1  |{\"Zipcode\":\"704\",\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n",
      "+---+----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.withColumn(\"value\",to_json(col(\"value\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7b4ae1ee-78ef-4cb3-9bff-3b29108c8b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+-----------+\n",
      "|id |Zipcode|ZipCodeType|City       |\n",
      "+---+-------+-----------+-----------+\n",
      "|1  |704    |STANDARD   |PARC PARQUE|\n",
      "+---+-------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"id\"),json_tuple(col(\"value\"),\"Zipcode\",\"ZipCodeType\",\"City\")) \\\n",
    "    .toDF(\"id\",\"Zipcode\",\"ZipCodeType\",\"City\") \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8df0f8a9-e873-4ec5-8341-cd3de715ab92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|id |ZipCodeType|\n",
      "+---+-----------+\n",
      "|1  |STANDARD   |\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"id\"),get_json_object(col(\"value\"),\"$.ZipCodeType\").alias(\"ZipCodeType\")) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4dbb9c7e-0585-47c9-a7c5-2d02baa2c53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRUCT<City: STRING, State: STRING, ZipCodeType: STRING, Zipcode: BIGINT>\n"
     ]
    }
   ],
   "source": [
    "schemaStr=spark.range(1) \\\n",
    "    .select(schema_of_json(lit(\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"))) \\\n",
    "    .collect()[0][0]\n",
    "print(schemaStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eab2df-3950-4258-9c24-9376886b1f76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
