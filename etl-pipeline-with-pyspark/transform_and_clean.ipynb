{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61e31b71",
   "metadata": {
    "id": "61e31b71"
   },
   "source": [
    "## **Step 2: Transforming and Cleaning Data**\n",
    "\n",
    "### **Background & Introduction**\n",
    "Data cleaning and transformation are critical steps in any ETL pipeline. Before data can be analyzed or loaded into storage systems, it must be structured, deduplicated, and properly formatted.\n",
    "\n",
    "In this step, you will perform key transformations to prepare the video streaming dataset for analysis. This includes handling missing values, deduplicating data, converting data types, enriching records with joins, and applying aggregations.\n",
    "\n",
    "---\n",
    "\n",
    "### **Objective:**\n",
    "Perform data cleaning, deduplication, type conversions, and transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2O5fHU5_C3O",
   "metadata": {
    "id": "c2O5fHU5_C3O"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/11 07:05:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"VideoStreamingETL\").getOrCreate()\n",
    "\n",
    "# Reduce verbose logging\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Configure logging to suppress warnings\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a79925b5",
   "metadata": {
    "id": "a79925b5"
   },
   "outputs": [],
   "source": [
    "# Define dataset paths\n",
    "viewing_history_path = \"video_streaming_data/viewing_history.csv\"\n",
    "users_path = \"video_streaming_data/users.json\"\n",
    "\n",
    "# Load data\n",
    "viewing_history_df = spark.read.option(\"header\", True).csv(viewing_history_path)\n",
    "users_df = spark.read.option(\"multiline\", \"true\").json(users_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4834dcc1",
   "metadata": {
    "id": "4834dcc1"
   },
   "source": [
    "### **Task 1: Handle Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9cd73cb",
   "metadata": {
    "id": "d9cd73cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Null counts in viewing_history_df:\n",
      "+-------+--------+----------+-----------+--------------+\n",
      "|user_id|video_id|watched_at|device_type|account_status|\n",
      "+-------+--------+----------+-----------+--------------+\n",
      "|      0|       0|         0|      34212|         34076|\n",
      "+-------+--------+----------+-----------+--------------+\n",
      "\n",
      "+-----+----+------------------+-----------------+-------+\n",
      "|email|name|preferred_language|subscription_date|user_id|\n",
      "+-----+----+------------------+-----------------+-------+\n",
      "|    0|   0|             25376|            34231|      0|\n",
      "+-----+----+------------------+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as _sum, when\n",
    "\n",
    "# Get the Null counts in viewing_history\n",
    "print(\" Null counts in viewing_history_df:\")\n",
    "viewing_history_df.select([\n",
    "    _sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "    for c in viewing_history_df.columns\n",
    "]).show()\n",
    "\n",
    "# Get the Null counts in users_df\n",
    "users_df.select([\n",
    "    _sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "    for c in users_df.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11dd3073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing device_type with \"Unknown\"\n",
    "viewing_history_df = viewing_history_df.na.fill(value=\"Unknown\", subset=[\"device_type\"])\n",
    "\n",
    "\n",
    "# Drop account_status column (mostly null)\n",
    "if \"account_status\" in viewing_history_df.columns:\n",
    "    viewing_history_df = viewing_history_df.drop(\"account_status\")\n",
    "\n",
    "# Drop rows missing user_id or watched_at (critical fields)\n",
    "viewing_history_df = viewing_history_df.dropna(subset=[\"user_id\", \"watched_at\"])\n",
    "\n",
    "# Fill preferred_language with \"Unknown\"\n",
    "users_df = users_df.fillna({\"preferred_language\": \"Unknown\"})\n",
    "\n",
    "# Fill missing subscription_date with a placeholder\n",
    "users_df = users_df.fillna({\"subscription_date\": \"2020-01-01\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88c16dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+-----------+\n",
      "|user_id|video_id|watched_at|device_type|\n",
      "+-------+--------+----------+-----------+\n",
      "|      0|       0|         0|          0|\n",
      "+-------+--------+----------+-----------+\n",
      "\n",
      "+-----+----+------------------+-----------------+-------+\n",
      "|email|name|preferred_language|subscription_date|user_id|\n",
      "+-----+----+------------------+-----------------+-------+\n",
      "|    0|   0|                 0|                0|      0|\n",
      "+-----+----+------------------+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify by rerunning\n",
    "\n",
    "from pyspark.sql.functions import col, sum as _sum, when\n",
    "\n",
    "# Get the Null counts in viewing_history\n",
    "viewing_history_df.select([\n",
    "    _sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "    for c in viewing_history_df.columns\n",
    "]).show()\n",
    "\n",
    "# Get the Null counts in users_df\n",
    "users_df.select([\n",
    "    _sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "    for c in users_df.columns\n",
    "]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462069df",
   "metadata": {
    "id": "462069df"
   },
   "source": [
    "### **Task 2: Remove Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4335e2d",
   "metadata": {
    "id": "e4335e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows = 102000\n",
      "Distinct rows = 100000\n",
      "Duplicate rows = 2000\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "users_total = users_df.count()\n",
    "unique_user_ids = users_df.select(\"user_id\").distinct().count()\n",
    "\n",
    "# Print total rows, distinct rows, and duplicate rows\n",
    "print(f\"Total rows = {users_total}\")\n",
    "print(f\"Distinct rows = {unique_user_ids}\")\n",
    "print(f\"Duplicate rows = {users_total - unique_user_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7580a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates based on user_id\n",
    "users_df = users_df.dropDuplicates([\"user_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37c5de62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|user_id|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confirm cleanup\n",
    "users_df.groupBy(\"user_id\").count().filter(col(\"count\") > 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baea485d",
   "metadata": {
    "id": "baea485d"
   },
   "source": [
    "### **Task 3: Convert Data Types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2801539",
   "metadata": {
    "id": "f2801539"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- watched_at: string (nullable = true)\n",
      " |-- device_type: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the current schema\n",
    "viewing_history_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b739d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col\n",
    "\n",
    "# Convert user_id to IntegerType\n",
    "viewing_history_df = viewing_history_df.withColumn(\"user_id\", col(\"user_id\").cast(\"int\"))\n",
    "\n",
    "# Convert watched_at to TimestampType\n",
    "viewing_history_df = viewing_history_df.withColumn(\"watched_at\", to_timestamp(col(\"watched_at\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bd13bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- watched_at: timestamp (nullable = true)\n",
      " |-- device_type: string (nullable = false)\n",
      "\n",
      "+-------+-------------------+\n",
      "|user_id|         watched_at|\n",
      "+-------+-------------------+\n",
      "|  21558|2024-01-01 00:00:00|\n",
      "| 190681|2024-01-01 00:00:01|\n",
      "| 190802|2024-01-01 00:00:02|\n",
      "|  40874|2024-01-01 00:00:03|\n",
      "|  92704|2024-01-01 00:00:04|\n",
      "+-------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confirm the changes\n",
    "viewing_history_df.printSchema()\n",
    "\n",
    "viewing_history_df.select(\"user_id\", \"watched_at\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96170e39",
   "metadata": {
    "id": "96170e39"
   },
   "source": [
    "### **Task 4: Filtering & Joins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96f236fb",
   "metadata": {
    "id": "96f236fb"
   },
   "outputs": [],
   "source": [
    "# Optional: Cast user_id in both dataframes\n",
    "\n",
    "viewing_history_df = viewing_history_df.withColumn(\"user_id\", col(\"user_id\").cast(\"int\"))\n",
    "\n",
    "users_df = users_df.withColumn(\"user_id\", col(\"user_id\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33b17148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|user_id|\n",
      "+-------+\n",
      "|21558  |\n",
      "|190681 |\n",
      "|190802 |\n",
      "|40874  |\n",
      "|92704  |\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+\n",
      "|user_id|\n",
      "+-------+\n",
      "| 849311|\n",
      "| 521357|\n",
      "| 400692|\n",
      "| 798647|\n",
      "| 869136|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make sure both DataFrames are clean and have user_id\n",
    "viewing_history_df.select(\"user_id\").show(5, truncate=False)\n",
    "users_df.select(\"user_id\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39bfee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join both DataFrames\n",
    "\n",
    "joined_df = viewing_history_df.join(users_df, on=\"user_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e87e405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+---------------------+------------------+\n",
      "|user_id|video_id|watched_at         |email                |preferred_language|\n",
      "+-------+--------+-------------------+---------------------+------------------+\n",
      "|10017  |18662   |2024-01-04 18:26:07|zachary06@example.com|Spanish           |\n",
      "|10017  |18485   |2024-01-01 01:41:54|zachary06@example.com|Spanish           |\n",
      "|10037  |16415   |2024-01-02 18:53:51|kyle00@example.org   |Unknown           |\n",
      "|10046  |7534    |2024-01-03 04:10:25|tracey42@example.org |English           |\n",
      "|10046  |1315    |2024-01-03 03:23:54|tracey42@example.org |English           |\n",
      "+-------+--------+-------------------+---------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview Results \n",
    "\n",
    "joined_df.select(\"user_id\", \"video_id\", \"watched_at\", \"email\", \"preferred_language\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712a38c0",
   "metadata": {
    "id": "712a38c0"
   },
   "source": [
    "### **Task 5: Aggregations & Window Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79354a85",
   "metadata": {
    "id": "79354a85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.262s][warning][os,thread] Failed to start thread \"Unknown thread\" - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.\n",
      "[14.264s][warning][os,thread] Failed to start the native thread for java.lang.Thread \"block-manager-ask-thread-pool-90\"\n",
      "[14.265s][warning][os,thread] Failed to start thread \"Unknown thread\" - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.\n",
      "[14.266s][warning][os,thread] Failed to start the native thread for java.lang.Thread \"block-manager-ask-thread-pool-91\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"block-manager-ask-thread-pool-89\" java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached\n",
      "\tat java.base/java.lang.Thread.start0(Native Method)\n",
      "\tat java.base/java.lang.Thread.start(Thread.java:1553)\n",
      "\tat java.base/java.lang.System$2.start(System.java:2573)\n",
      "\tat java.base/jdk.internal.vm.SharedThreadContainer.start(SharedThreadContainer.java:152)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:953)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:1021)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1158)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.765s][warning][os,thread] Failed to start thread \"Unknown thread\" - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.\n",
      "[14.766s][warning][os,thread] Failed to start the native thread for java.lang.Thread \"broadcast-exchange-1\"\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o262.showString.\n: java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached\n\tat java.base/java.lang.Thread.start0(Native Method)\n\tat java.base/java.lang.Thread.start(Thread.java:1553)\n\tat java.base/java.lang.System$2.start(System.java:2573)\n\tat java.base/jdk.internal.vm.SharedThreadContainer.start(SharedThreadContainer.java:152)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:953)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1364)\n\tat java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)\n\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.submit(ExecutionContextImpl.scala:144)\n\tat org.apache.spark.sql.execution.SQLExecution$.withThreadLocalCaptured(SQLExecution.scala:219)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.relationFuture$lzycompute(BroadcastExchangeExec.scala:134)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.relationFuture(BroadcastExchangeExec.scala:131)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doPrepare(BroadcastExchangeExec.scala:203)\n\tat org.apache.spark.sql.execution.SparkPlan.prepare(SparkPlan.scala:295)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:244)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeLike.submitBroadcastJob(BroadcastExchangeExec.scala:68)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeLike.submitBroadcastJob$(BroadcastExchangeExec.scala:67)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.submitBroadcastJob(BroadcastExchangeExec.scala:83)\n\tat org.apache.spark.sql.execution.adaptive.BroadcastQueryStageExec.doMaterialize(QueryStageExec.scala:248)\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:61)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:302)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:300)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:300)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:419)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m most_watched_df \u001b[38;5;241m=\u001b[39m joined_df\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39magg(\n\u001b[1;32m      8\u001b[0m     count(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwatch_count\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Show most watched videos overall\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmost_watched_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morderBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwatch_count\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mascending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pyspark/sql/dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \n\u001b[1;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pyspark/sql/dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    962\u001b[0m     )\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o262.showString.\n: java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached\n\tat java.base/java.lang.Thread.start0(Native Method)\n\tat java.base/java.lang.Thread.start(Thread.java:1553)\n\tat java.base/java.lang.System$2.start(System.java:2573)\n\tat java.base/jdk.internal.vm.SharedThreadContainer.start(SharedThreadContainer.java:152)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:953)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1364)\n\tat java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)\n\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.submit(ExecutionContextImpl.scala:144)\n\tat org.apache.spark.sql.execution.SQLExecution$.withThreadLocalCaptured(SQLExecution.scala:219)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.relationFuture$lzycompute(BroadcastExchangeExec.scala:134)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.relationFuture(BroadcastExchangeExec.scala:131)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doPrepare(BroadcastExchangeExec.scala:203)\n\tat org.apache.spark.sql.execution.SparkPlan.prepare(SparkPlan.scala:295)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:244)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeLike.submitBroadcastJob(BroadcastExchangeExec.scala:68)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeLike.submitBroadcastJob$(BroadcastExchangeExec.scala:67)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.submitBroadcastJob(BroadcastExchangeExec.scala:83)\n\tat org.apache.spark.sql.execution.adaptive.BroadcastQueryStageExec.doMaterialize(QueryStageExec.scala:248)\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:61)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:302)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:300)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:300)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:419)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Count Views per User per Video\n",
    "joined_df.groupBy(\"video_id\").agg(count(\"*\").alias(\"views_per_video\"))\n",
    "\n",
    "# Count how many times each user watched each video\n",
    "most_watched_df = joined_df.groupBy(\"user_id\", \"video_id\").agg(\n",
    "    count(\"*\").alias(\"watch_count\")\n",
    ")\n",
    "\n",
    "# Show most watched videos overall\n",
    "most_watched_df.orderBy(\"watch_count\", ascending=False).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc88947",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Total Views per User\n",
    "# Count total watch events per user\n",
    "total_views_df = joined_df.groupBy(\"user_id\").agg(\n",
    "    count(\"*\").alias(\"total_views\")\n",
    ")\n",
    "\n",
    "# Show most active users\n",
    "total_views_df.orderBy(\"total_views\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9dde1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rank Top 3 Most-Watched Videos per User\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Create window spec to rank videos within each user\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(col(\"watch_count\").desc())\n",
    "\n",
    "# Add ranking column\n",
    "ranked_df = most_watched_df.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "# Filter to get top 3 videos per user\n",
    "top_videos_per_user = ranked_df.filter(col(\"rank\") <= 3)\n",
    "\n",
    "# Show results\n",
    "top_videos_per_user.orderBy(\"user_id\", \"rank\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab503973",
   "metadata": {
    "id": "ab503973"
   },
   "source": [
    "### **Task 6: Apply UDFs & Vectorized UDFs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb1fdd6",
   "metadata": {
    "id": "2bb1fdd6"
   },
   "outputs": [],
   "source": [
    "# Preview device types\n",
    "viewing_history_df.select(\"device_type\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5668824",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a Python UDF to clean device labels\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define the normalization logic\n",
    "def normalize_device(device):\n",
    "    if device is None:\n",
    "        return \"Unknown\"\n",
    "    device = device.lower()\n",
    "    if \"iphone\" in device:\n",
    "        return \"iPhone\"\n",
    "    elif \"android\" in device:\n",
    "        return \"Android\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "# Register as a PySpark UDF\n",
    "normalize_device_udf = udf(normalize_device, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a546c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the UDF to your DataFrame\n",
    "# Create a new column with normalized values\n",
    "viewing_history_df = viewing_history_df.withColumn(\n",
    "    \"normalized_device\", normalize_device_udf(viewing_history_df[\"device_type\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d584f59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.259s][warning][os,thread] Failed to start thread \"Unknown thread\" - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.\n",
      "[16.261s][warning][os,thread] Failed to start the native thread for java.lang.Thread \"read stdout for getconf\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 07:05:52 ERROR Utils: Uncaught exception in thread executor-heartbeater\n",
      "java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached\n",
      "\tat java.base/java.lang.Thread.start0(Native Method)\n",
      "\tat java.base/java.lang.Thread.start(Thread.java:1526)\n",
      "\tat org.apache.spark.util.Utils$.processStreamByLine(Utils.scala:1327)\n",
      "\tat org.apache.spark.util.Utils$.executeAndGetOutput(Utils.scala:1302)\n",
      "\tat org.apache.spark.executor.ProcfsMetricsGetter.computePageSize(ProcfsMetricsGetter.scala:93)\n",
      "\tat org.apache.spark.executor.ProcfsMetricsGetter.<init>(ProcfsMetricsGetter.scala:47)\n",
      "\tat org.apache.spark.executor.ProcfsMetricsGetter$.<init>(ProcfsMetricsGetter.scala:233)\n",
      "\tat org.apache.spark.executor.ProcfsMetricsGetter$.<clinit>(ProcfsMetricsGetter.scala)\n",
      "\tat org.apache.spark.metrics.ProcessTreeMetrics$.getMetricValues(ExecutorMetricType.scala:93)\n",
      "\tat org.apache.spark.executor.ExecutorMetrics$.$anonfun$getCurrentMetrics$1(ExecutorMetrics.scala:103)\n",
      "\tat org.apache.spark.executor.ExecutorMetrics$.$anonfun$getCurrentMetrics$1$adapted(ExecutorMetrics.scala:102)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.executor.ExecutorMetrics$.getCurrentMetrics(ExecutorMetrics.scala:102)\n",
      "\tat org.apache.spark.executor.ExecutorMetricsPoller.poll(ExecutorMetricsPoller.scala:82)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1197)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "26/01/11 07:05:56 ERROR Utils: Uncaught exception in thread driver-heartbeater\n",
      "java.lang.NoClassDefFoundError: Could not initialize class org.apache.spark.executor.ProcfsMetricsGetter$\n",
      "\tat org.apache.spark.metrics.ProcessTreeMetrics$.getMetricValues(ExecutorMetricType.scala:93)\n",
      "\tat org.apache.spark.executor.ExecutorMetrics$.$anonfun$getCurrentMetrics$1(ExecutorMetrics.scala:103)\n",
      "\tat org.apache.spark.executor.ExecutorMetrics$.$anonfun$getCurrentMetrics$1$adapted(ExecutorMetrics.scala:102)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.executor.ExecutorMetrics$.getCurrentMetrics(ExecutorMetrics.scala:102)\n",
      "\tat org.apache.spark.SparkContext.reportHeartBeat(SparkContext.scala:2781)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$23(SparkContext.scala:592)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.lang.ExceptionInInitializerError: Exception java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached [in thread \"executor-heartbeater\"]\n",
      "\tat java.base/java.lang.Thread.start0(Native Method)\n",
      "\tat java.base/java.lang.Thread.start(Thread.java:1526)\n",
      "\tat org.apache.spark.util.Utils$.processStreamByLine(Utils.scala:1327)\n",
      "\tat org.apache.spark.util.Utils$.executeAndGetOutput(Utils.scala:1302)\n",
      "\tat org.apache.spark.executor.ProcfsMetricsGetter.computePageSize(ProcfsMetricsGetter.scala:93)\n",
      "\tat org.apache.spark.executor.ProcfsMetricsGetter.<init>(ProcfsMetricsGetter.scala:47)\n",
      "\tat org.apache.spark.executor.ProcfsMetricsGetter$.<init>(ProcfsMetricsGetter.scala:233)\n",
      "\tat org.apache.spark.executor.ProcfsMetricsGetter$.<clinit>(ProcfsMetricsGetter.scala)\n",
      "\tat org.apache.spark.metrics.ProcessTreeMetrics$.getMetricValues(ExecutorMetricType.scala:93)\n",
      "\tat org.apache.spark.executor.ExecutorMetrics$.$anonfun$getCurrentMetrics$1(ExecutorMetrics.scala:103)\n",
      "\tat org.apache.spark.executor.ExecutorMetrics$.$anonfun$getCurrentMetrics$1$adapted(ExecutorMetrics.scala:102)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.executor.ExecutorMetrics$.getCurrentMetrics(ExecutorMetrics.scala:102)\n",
      "\tat org.apache.spark.executor.ExecutorMetricsPoller.poll(ExecutorMetricsPoller.scala:82)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1197)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\t... 9 more\n"
     ]
    }
   ],
   "source": [
    "# Inspect the result\n",
    "viewing_history_df.select(\"device_type\", \"normalized_device\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b61c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
