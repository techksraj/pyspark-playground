{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvgH6SOXAQvJ"
   },
   "source": [
    "# Step 1: Extracting Data from Multiple Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAwg-KSo_OSX"
   },
   "source": [
    "### Background & Setup\n",
    "This notebook is designed to guide you through the extraction of data from multiple sources using PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0q4rzyg_Qxj"
   },
   "source": [
    "**Required Libraries:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywitLjgs_ZFI"
   },
   "source": [
    "**Import necessary libraries and Start Spark Session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tGh81MaD_Hcz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/10 23:48:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"VideoStreamingETL\").getOrCreate()\n",
    "\n",
    "# Reduce verbose logging\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Configure logging to suppress warnings\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5OKvn2z_lRF"
   },
   "source": [
    "**Define dataset paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0NPiDsof_LBE"
   },
   "outputs": [],
   "source": [
    "# Define dataset paths\n",
    "viewing_history_path = \"video_streaming_data/viewing_history.csv\"\n",
    "users_path = \"video_streaming_data/users.json\"\n",
    "videos_catalog_path = \"video_streaming_data/videos_catalog.parquet\"\n",
    "subscription_updates_path = \"video_streaming_data/subscription_updates.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kB_hV4xV_rz9"
   },
   "source": [
    "## Task 1: Load CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JnE3misD_oMv"
   },
   "outputs": [],
   "source": [
    "# Load the CSV file using Spark\n",
    "df = spark.read.options(header='true', inferSchema='true').csv(viewing_history_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- video_id: integer (nullable = true)\n",
      " |-- watched_at: timestamp (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- account_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+-----------+--------------+\n",
      "|user_id|video_id|watched_at         |device_type|account_status|\n",
      "+-------+--------+-------------------+-----------+--------------+\n",
      "|21558  |10308   |2024-01-01 00:00:00|iPhoneX    |inactive      |\n",
      "|190681 |6220    |2024-01-01 00:00:01|iPhoneX    |inactive      |\n",
      "|190802 |3119    |2024-01-01 00:00:02|iPhoneX    |active        |\n",
      "|40874  |14184   |2024-01-01 00:00:03|iPhoneX    |active        |\n",
      "|92704  |13010   |2024-01-01 00:00:04|iPhoneX    |NULL          |\n",
      "|134289 |4249    |2024-01-01 00:00:05|Android    |inactive      |\n",
      "|82958  |14470   |2024-01-01 00:00:06|Android    |active        |\n",
      "|25131  |3244    |2024-01-01 00:00:07|NULL       |NULL          |\n",
      "|144215 |19549   |2024-01-01 00:00:08|NULL       |inactive      |\n",
      "|146331 |6001    |2024-01-01 00:00:09|Windows    |active        |\n",
      "|174960 |12087   |2024-01-01 00:00:10|Windows    |NULL          |\n",
      "|143062 |6082    |2024-01-01 00:00:11|iPhoneX    |active        |\n",
      "|173784 |19999   |2024-01-01 00:00:12|Android    |active        |\n",
      "|137330 |16604   |2024-01-01 00:00:13|Android    |active        |\n",
      "|193391 |15363   |2024-01-01 00:00:14|iPhoneX    |active        |\n",
      "|113760 |17835   |2024-01-01 00:00:15|iPhoneX    |active        |\n",
      "|122733 |7399    |2024-01-01 00:00:16|Android    |active        |\n",
      "|93020  |18840   |2024-01-01 00:00:17|Windows    |active        |\n",
      "|133875 |12728   |2024-01-01 00:00:18|Android    |active        |\n",
      "|56906  |13472   |2024-01-01 00:00:19|Windows    |inactive      |\n",
      "+-------+--------+-------------------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview the data\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5wuct1A_-2r"
   },
   "source": [
    "## Task 2: Load JSON Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Xss9UIT2_6ta"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the JSON file using multiline mode\n",
    "df_json = spark.read.option(\"multiline\",\"true\").json(users_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- email: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- preferred_language: string (nullable = true)\n",
      " |-- subscription_date: string (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview the schema\n",
    "df_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------------------+-----------------+-------+\n",
      "|               email|               name|preferred_language|subscription_date|user_id|\n",
      "+--------------------+-------------------+------------------+-----------------+-------+\n",
      "|jenniferblack@exa...|        John Duncan|           Spanish|       2022-12-01| 180981|\n",
      "|johnbarker@exampl...|     Tammy Thompson|           English|             NULL| 244816|\n",
      "|esherman@example.com|        Andrew Vega|            French|       2023-06-15| 619605|\n",
      "|melissarivera@exa...|       Melissa Sims|            French|             NULL| 342762|\n",
      "|gcopeland@example...|    Yvonne Anderson|           Spanish|       2023-06-15| 236364|\n",
      "|griffinmichelle@e...|   Gregory Williams|            French|       2023-06-15| 257955|\n",
      "|wilsonsandra@exam...|     Kimberly White|           English|             NULL| 845355|\n",
      "|robertmedina@exam...|  Justin Cunningham|           English|       2023-06-15| 899823|\n",
      "|xschwartz@example...|       Julie Harvey|            French|             NULL| 962191|\n",
      "| james13@example.net|        Tracy Davis|           English|       2022-12-01|  66790|\n",
      "|jessica79@example...|  Christopher Ochoa|            French|       2022-12-01| 592639|\n",
      "|   anash@example.com|      Chelsea Jones|              NULL|       2023-06-15| 597082|\n",
      "|christian86@examp...|   Alexander Riddle|           Spanish|       2023-06-15| 285563|\n",
      "|garciajamie@examp...|Nathaniel Valentine|           Spanish|       2023-06-15| 474458|\n",
      "| david51@example.com|     Cynthia Harris|              NULL|             NULL| 306103|\n",
      "|faithmcconnell@ex...|       Robert Hicks|            French|             NULL| 671616|\n",
      "|matthewwebb@examp...|      Jennifer Wang|              NULL|       2022-12-01| 787032|\n",
      "|donaldgood@exampl...|        Mary Graves|           English|             NULL| 721322|\n",
      "|colleen02@example...|    Michael Johnson|           English|       2022-12-01| 971770|\n",
      "|millerjenna@examp...|      Andrew Burton|              NULL|             NULL| 568022|\n",
      "+--------------------+-------------------+------------------+-----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview the data\n",
    "df_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzP57Gu7_zM8"
   },
   "source": [
    "## Task 3: Load Parquet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ixMXi4YxAAsy"
   },
   "outputs": [],
   "source": [
    "# Read the Parquet file\n",
    "df_parquet = spark.read.parquet(videos_catalog_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview the schema\n",
    "df_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+\n",
      "|video_id|     title|category_id|\n",
      "+--------+----------+-----------+\n",
      "|    1000|Video_1000|          2|\n",
      "|    1001|Video_1001|          2|\n",
      "|    1002|Video_1002|          3|\n",
      "|    1003|Video_1003|          1|\n",
      "|    1004|Video_1004|          1|\n",
      "|    1005|Video_1005|          1|\n",
      "|    1006|Video_1006|          2|\n",
      "|    1007|Video_1007|          1|\n",
      "|    1008|Video_1008|    unknown|\n",
      "|    1009|Video_1009|          1|\n",
      "|    1010|Video_1010|          1|\n",
      "|    1011|Video_1011|          2|\n",
      "|    1012|Video_1012|          1|\n",
      "|    1013|Video_1013|    unknown|\n",
      "|    1014|Video_1014|          3|\n",
      "|    1015|Video_1015|          2|\n",
      "|    1016|Video_1016|    unknown|\n",
      "|    1017|Video_1017|          1|\n",
      "|    1018|Video_1018|          3|\n",
      "|    1019|Video_1019|          2|\n",
      "+--------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview the data\n",
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wul-FfpADqm"
   },
   "source": [
    "## Task 4: Extract Incremental Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "9wlUddW3AFtj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+-------------------+\n",
      "|user_id|subscription_status|        change_date|cancellation_reason|\n",
      "+-------+-------------------+-------------------+-------------------+\n",
      "| 130320|             active|2024-02-01 00:00:00|            content|\n",
      "| 159457|             active|2024-02-01 00:01:00|            content|\n",
      "| 158158|             active|2024-02-01 00:02:00|              price|\n",
      "|  61946|            renewed|2024-02-01 00:03:00|              price|\n",
      "| 170368|            renewed|2024-02-01 00:04:00|            content|\n",
      "|  66434|             active|2024-02-01 00:05:00|               NULL|\n",
      "|  96212|          cancelled|2024-02-01 00:06:00|            content|\n",
      "| 103617|          cancelled|2024-02-01 00:07:00|               NULL|\n",
      "| 132397|            renewed|2024-02-01 00:08:00|            content|\n",
      "|  32356|            renewed|2024-02-01 00:09:00|              other|\n",
      "|  39990|             active|2024-02-01 00:10:00|              price|\n",
      "| 113447|          cancelled|2024-02-01 00:11:00|              other|\n",
      "|  46143|             active|2024-02-01 00:12:00|              price|\n",
      "| 118016|          cancelled|2024-02-01 00:13:00|            content|\n",
      "| 147765|             active|2024-02-01 00:14:00|              other|\n",
      "|  37155|             active|2024-02-01 00:15:00|              price|\n",
      "|  92063|             active|2024-02-01 00:16:00|               NULL|\n",
      "|  89463|             active|2024-02-01 00:17:00|            content|\n",
      "| 171364|             active|2024-02-01 00:18:00|              price|\n",
      "| 164954|             active|2024-02-01 00:19:00|              other|\n",
      "+-------+-------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Enable header and schema inference:\n",
    "df_sub = spark.read.options(inferSchema=\"true\", header=\"true\").csv(subscription_updates_path)\n",
    "df_sub.show()\n",
    "\n",
    "# Extract only records from February 7, 2024 onwards\n",
    "df_sub = df_sub.filter(col(\"change_date\") >= \"2024-02-07\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+-------------------+\n",
      "|user_id|subscription_status|        change_date|cancellation_reason|\n",
      "+-------+-------------------+-------------------+-------------------+\n",
      "| 160619|          cancelled|2024-02-07 00:00:00|              price|\n",
      "|  14529|            renewed|2024-02-07 00:01:00|              other|\n",
      "| 105150|            renewed|2024-02-07 00:02:00|              other|\n",
      "|  86291|             active|2024-02-07 00:03:00|              price|\n",
      "| 162671|             active|2024-02-07 00:04:00|            content|\n",
      "|  40590|             active|2024-02-07 00:05:00|               NULL|\n",
      "|  96689|             active|2024-02-07 00:06:00|              price|\n",
      "|  63004|            renewed|2024-02-07 00:07:00|               NULL|\n",
      "|  16426|             active|2024-02-07 00:08:00|              price|\n",
      "| 110566|             active|2024-02-07 00:09:00|              price|\n",
      "| 148138|             active|2024-02-07 00:10:00|              price|\n",
      "| 133143|             active|2024-02-07 00:11:00|            content|\n",
      "|  52362|             active|2024-02-07 00:12:00|              price|\n",
      "| 115774|          cancelled|2024-02-07 00:13:00|            content|\n",
      "|  19196|            renewed|2024-02-07 00:14:00|            content|\n",
      "|  73287|            renewed|2024-02-07 00:15:00|              other|\n",
      "|  75498|             active|2024-02-07 00:16:00|               NULL|\n",
      "| 176700|             active|2024-02-07 00:17:00|            content|\n",
      "|  97475|          cancelled|2024-02-07 00:18:00|            content|\n",
      "|  47744|            renewed|2024-02-07 00:19:00|              price|\n",
      "+-------+-------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview the data\n",
    "df_sub.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
